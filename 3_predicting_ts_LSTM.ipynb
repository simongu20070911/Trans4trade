{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-10-15T10:46:48.967291Z",
     "iopub.status.busy": "2024-10-15T10:46:48.966948Z",
     "iopub.status.idle": "2024-10-15T10:46:54.208571Z",
     "shell.execute_reply": "2024-10-15T10:46:54.208019Z",
     "shell.execute_reply.started": "2024-10-15T10:46:48.967269Z"
    },
    "id": "z68p_q4eISQP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from tqdm import trange\n",
    "import random\n",
    "import math\n",
    "\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-10-15T10:46:54.210006Z",
     "iopub.status.busy": "2024-10-15T10:46:54.209662Z",
     "iopub.status.idle": "2024-10-15T10:46:54.216528Z",
     "shell.execute_reply": "2024-10-15T10:46:54.215979Z",
     "shell.execute_reply.started": "2024-10-15T10:46:54.209985Z"
    },
    "id": "FUcYIRwMIVPV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"plots\": {\n",
    "        \"show_plots\": False,\n",
    "        \"xticks_interval\": 1200,\n",
    "        \"color_actual\": \"#001f3f\",\n",
    "        \"color_train\": \"#3D9970\",\n",
    "        \"color_val\": \"#0074D9\",\n",
    "        \"color_test\": \"#FF4136\",\n",
    "        \"color_pred_train\": \"#3D9970\",\n",
    "        \"color_pred_val\": \"#0074D9\",\n",
    "        \"color_pred_test\": \"#FF4136\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"window_size\": 10,\n",
    "        \"train_split_size\": 0.80,\n",
    "    }, \n",
    "    \"model_MO\": {\n",
    "        \"input_size\": 1, # since we are only using 1 feature, price\n",
    "        \"num_lstm_layers\": 5,\n",
    "        \"lstm_size\": 16,\n",
    "        \"dropout\": 0.25,\n",
    "        \"output_size\": 1,\n",
    "    },\n",
    "    \"training_MO\": {\n",
    "        \"batch_size\": 1,\n",
    "        \"num_epoch\": 50,\n",
    "        \"learning_rate\": 0.005,\n",
    "        \"scheduler_step_size\": 10,\n",
    "    },\n",
    "    \"model_MM\": {\n",
    "        \"input_size\": 1, # since we are only using 1 feature, price\n",
    "        \"num_lstm_layers\": 5,\n",
    "        \"lstm_size\": 16,\n",
    "        \"dropout\": 0.4,\n",
    "        \"target_len\": 60,\n",
    "        \"stride\": 1,\n",
    "        \"patience\": 5, # number of epochs to wait for Early Stopping\n",
    "    },\n",
    "    \"training_MM\": {\n",
    "        \"batch_size\": 256,\n",
    "        \"num_epoch\": 15,\n",
    "        \"learning_rate\": 0.004,\n",
    "        \"scheduler_step_size\": 10,\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"drive\": {\n",
    "            \"agg_trade\": {\n",
    "                \"train\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/input_data/\",\n",
    "                \"test\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/input_data/\", \n",
    "            },\n",
    "            \"orderbook\": {\n",
    "                \"train\": \"/content/drive/MyDrive/IP/Repos/LSTM_Transformer/input_data/orderbook_clean.csv\",\n",
    "                \"test\": \"/content/drive/MyDrive/IP/Repos/LSTM_Transformer/input_data/orderbook_test_clean.csv\",\n",
    "            },\n",
    "            \"models\": \"/content/drive/MyDrive/IP/Repos/LSTM_Transformer/models/\",\n",
    "            \"figures\": \"/content/drive/MyDrive/IP/Repos/LSTM_Transformer/figures/\",\n",
    "        },\n",
    "        \"local\": {\n",
    "            \"agg_trade\": {\n",
    "                \"train\": \"./input_data/\",\n",
    "                \"test\": \"./input_data/\", \n",
    "            },\n",
    "            \"orderbook\": {\n",
    "                \"train\": \"./input_data/orderbook_clean.csv\",\n",
    "                \"test\": \"./input_data/orderbook_test_clean.csv\",\n",
    "            },\n",
    "            \"models\": \"./models/\",\n",
    "            \"figures\": \"./figures\",\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olYwt1apSFb6"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-15T10:46:54.217523Z",
     "iopub.status.busy": "2024-10-15T10:46:54.217205Z",
     "iopub.status.idle": "2024-10-15T10:46:56.433211Z",
     "shell.execute_reply": "2024-10-15T10:46:56.432613Z",
     "shell.execute_reply.started": "2024-10-15T10:46:54.217504Z"
    },
    "id": "gNUPSKJMEJLw",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "drive = False\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-15T10:46:56.434807Z",
     "iopub.status.busy": "2024-10-15T10:46:56.434539Z",
     "iopub.status.idle": "2024-10-15T10:46:56.437157Z",
     "shell.execute_reply": "2024-10-15T10:46:56.436676Z",
     "shell.execute_reply.started": "2024-10-15T10:46:56.434785Z"
    },
    "id": "JSphepU9DzNq",
    "outputId": "f970c93c-8584-4a72-d4d9-72251a1218c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T10:46:56.438048Z",
     "iopub.status.busy": "2024-10-15T10:46:56.437818Z",
     "iopub.status.idle": "2024-10-15T10:47:09.307513Z",
     "shell.execute_reply": "2024-10-15T10:47:09.306948Z",
     "shell.execute_reply.started": "2024-10-15T10:46:56.438027Z"
    },
    "id": "j7bWYyArIVRq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "date_train = 'All_to_Sept' \n",
    "date_test = 'All_to_Sept'\n",
    "drive = None\n",
    "if drive:\n",
    "    agg_trade = pd.read_csv(config[\"paths\"][\"drive\"][\"agg_trade\"][\"train\"]+date_train+'/orderbook.csv')    \n",
    "else:\n",
    "    agg_trade = pd.read_csv(config[\"paths\"][\"local\"][\"agg_trade\"][\"train\"]+date_train+'/orderbook_agg_trade_dollarvol.csv')\n",
    "    agg_trade_test = pd.read_csv(config[\"paths\"][\"local\"][\"agg_trade\"][\"test\"]+date_test+'/orderbook_agg_trade_dollarvol.csv')\n",
    "\n",
    "#agg_trade['price'] = agg_trade['w_midprice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mstDxZxSFb9"
   },
   "source": [
    "## Preparing Data for LSTM MO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T10:47:09.308553Z",
     "iopub.status.busy": "2024-10-15T10:47:09.308360Z",
     "iopub.status.idle": "2024-10-15T10:47:09.317173Z",
     "shell.execute_reply": "2024-10-15T10:47:09.316684Z",
     "shell.execute_reply.started": "2024-10-15T10:47:09.308534Z"
    },
    "id": "5cinhNa4JRHu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data_x(data, window_size, lag):\n",
    "    '''\n",
    "    Windows the input data for the ML models.\n",
    "    '''\n",
    "    n_row = data.shape[0] - window_size + 1\n",
    "    subset = data[:window_size]\n",
    "    subset_mean = np.mean(subset, axis=0)\n",
    "    output = np.zeros([n_row, window_size, len(subset_mean)])\n",
    "    x_mean = np.zeros([n_row, len(subset_mean)])\n",
    "    x_std = np.zeros([n_row, len(subset_mean)])\n",
    "    for idx in range(n_row):\n",
    "        subset = data[idx:idx+window_size]\n",
    "        subset_mean = np.mean(subset, axis=0)\n",
    "        subset_std = np.std(subset, axis=0) + 0.01\n",
    "        subset_norm = (subset-subset_mean)/subset_std\n",
    "        x_mean[idx,:] = subset_mean\n",
    "        x_std[idx,:] = subset_std\n",
    "        output[idx,:,:] = subset_norm\n",
    "    x_mean = np.array(x_mean)\n",
    "    x_std = np.array(x_std)\n",
    "    return output[:-lag-1], output[-1], x_mean, x_std\n",
    "\n",
    "def prepare_data_y(x, window_size, lag):\n",
    "    '''\n",
    "    Windows the target data for the ML models.\n",
    "    '''\n",
    "    output = np.zeros([len(x)-window_size-lag])\n",
    "    std = 1.1*np.sqrt(lag)+lag*0.01\n",
    "    for idx in range(0,len(x)-window_size-lag):\n",
    "        output[idx] = np.log(x[window_size+lag-1+idx,0]/x[window_size-1+idx,0])*10_000\n",
    "    output = output/std\n",
    "    return output\n",
    "\n",
    "def prepare_data(normalized_prices_train, dates_train, normalized_prices_test, dates_test, config, lag=1, plot=False):\n",
    "    '''\n",
    "    Returns input and target data.\n",
    "    '''\n",
    "    data_x, data_x_unseen, x_mean, x_std = prepare_data_x(normalized_prices_train, window_size=config[\"data\"][\"window_size\"], lag=lag)\n",
    "    data_y = prepare_data_y(normalized_prices_train, window_size=config[\"data\"][\"window_size\"], lag=lag)\n",
    "\n",
    "    split_index = int(data_y.shape[0]*config[\"data\"][\"train_split_size\"])\n",
    "    data_x_train = data_x[:split_index]\n",
    "    data_x_val = data_x[split_index:]\n",
    "    data_y_train = data_y[:split_index]\n",
    "    data_y_val = data_y[split_index:]\n",
    "\n",
    "    return split_index, data_x_train, data_y_train, data_x_val, data_y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7OaAEH26Dd48"
   },
   "source": [
    "## Preparing Data for LSTM MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T10:47:09.318098Z",
     "iopub.status.busy": "2024-10-15T10:47:09.317844Z",
     "iopub.status.idle": "2024-10-15T10:47:09.325507Z",
     "shell.execute_reply": "2024-10-15T10:47:09.325052Z",
     "shell.execute_reply.started": "2024-10-15T10:47:09.318080Z"
    },
    "id": "0f1c9FqDDd48",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following helper functions and models were based on existing code: https://github.com/lkulowski/LSTM_encoder_decoder\n",
    "\n",
    "def train_test_split(datetime, price, split = 0.8):\n",
    "  '''\n",
    "  Splits time series into train/test sets.\n",
    "  '''  \n",
    "  datetime = np.array(datetime)\n",
    "  price = np.array(price)\n",
    "  \n",
    "  indx_split = int(split * len(price))\n",
    "  indx_train = np.arange(0, indx_split, dtype=int)\n",
    "  indx_val = np.arange(indx_split, len(price), dtype=int)\n",
    "    \n",
    "  t_train = datetime[indx_train]\n",
    "  y_train = price[indx_train]\n",
    "  y_train = y_train.reshape(-1, 1)\n",
    "  \n",
    "  t_val = datetime[indx_val]\n",
    "  y_val = price[indx_val]\n",
    "  y_val = y_val.reshape(-1, 1)\n",
    "  \n",
    "  return t_train, y_train, t_val, y_val \n",
    "\n",
    "\n",
    "def windowed_dataset(y, input_window = 5, output_window = 1, stride = 1, num_features = 1):\n",
    "    '''\n",
    "    Creates a windowed dataset.\n",
    "    '''\n",
    "    L = y.shape[0]\n",
    "    num_samples = (L - input_window - output_window) // stride + 1\n",
    "\n",
    "    X = np.zeros([input_window, num_samples, num_features])\n",
    "    Y = np.zeros([output_window, num_samples, num_features])    \n",
    "    \n",
    "    for ff in np.arange(num_features):\n",
    "        for ii in np.arange(num_samples):\n",
    "            start_x = stride * ii\n",
    "            end_x = start_x + input_window\n",
    "            X[:, ii, ff] = y[start_x:end_x, ff]\n",
    "\n",
    "            start_y = stride * ii + input_window\n",
    "            end_y = start_y + output_window \n",
    "            Y[:, ii, ff] = y[start_y:end_y, ff]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def numpy_to_torch(x_train, y_train, x_validation, y_validation, device):\n",
    "    '''\n",
    "    Converts numpy array to PyTorch tensor.\n",
    "    '''\n",
    "    x_train_torch = torch.from_numpy(x_train).type(torch.Tensor).to(device)\n",
    "    y_train_torch = torch.from_numpy(y_train).type(torch.Tensor).to(device)\n",
    "\n",
    "    x_test_torch = torch.from_numpy(x_validation).type(torch.Tensor).to(device)\n",
    "    y_test_torch = torch.from_numpy(y_validation).type(torch.Tensor).to(device)\n",
    "    \n",
    "    return x_train_torch, y_train_torch, x_test_torch, y_test_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nW8JIIAzSFcA"
   },
   "source": [
    "## Defining the Many-to-One LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T10:47:09.326355Z",
     "iopub.status.busy": "2024-10-15T10:47:09.326184Z",
     "iopub.status.idle": "2024-10-15T10:47:09.329769Z",
     "shell.execute_reply": "2024-10-15T10:47:09.329307Z",
     "shell.execute_reply.started": "2024-10-15T10:47:09.326338Z"
    },
    "id": "1pRFMUlyNZvs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T10:47:09.331481Z",
     "iopub.status.busy": "2024-10-15T10:47:09.331234Z",
     "iopub.status.idle": "2024-10-15T10:47:09.344842Z",
     "shell.execute_reply": "2024-10-15T10:47:09.344372Z",
     "shell.execute_reply.started": "2024-10-15T10:47:09.331463Z"
    },
    "id": "6ynCp-tnJROi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTM_MO(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=32, num_layers=2, output_size=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        self.linear_1 = nn.Linear(input_size, hidden_layer_size)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.lstm = nn.LSTM(hidden_layer_size, hidden_size=self.hidden_layer_size, num_layers=num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(num_layers*hidden_layer_size, output_size)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                 nn.init.constant_(param, 0.0)\n",
    "            elif 'weight_ih' in name:\n",
    "                 nn.init.kaiming_normal_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                 nn.init.orthogonal_(param)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        x = self.linear_1(x)\n",
    "        x = self.prelu(x)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        x = h_n.permute(1, 0, 2).reshape(batchsize, -1) \n",
    "        x = self.dropout(x)\n",
    "        predictions = self.linear_2(x)\n",
    "        return predictions[:,-1]\n",
    "\n",
    "\n",
    "    def train_model(self, train_dataloader, val_dataloader, learning_rate, scheduler_step_size, n_epochs=50, device=\"cpu\", save_path=None, forecast_window=None):\n",
    "      \n",
    "        def run_epoch(dataloader, is_training=False):\n",
    "            epoch_loss = 0\n",
    "            outputs = torch.Tensor(0).to(device)\n",
    "            targets = torch.Tensor(0).to(device)\n",
    "            if is_training:\n",
    "                self.train()\n",
    "            else:\n",
    "                self.eval()\n",
    "            for idx, (x, y) in enumerate(dataloader):\n",
    "                if is_training:\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                batchsize = x.shape[0]\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                out = self.forward(x)\n",
    "                loss = criterion(out.contiguous(), y.contiguous())\n",
    "\n",
    "                if is_training:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                if not is_training:\n",
    "                    outputs = torch.cat((outputs.contiguous(), out))\n",
    "                    targets = torch.cat((targets, y.contiguous()))\n",
    "                    \n",
    "                epoch_loss += (loss.detach().item() / batchsize)\n",
    "                \n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            if not is_training:\n",
    "                print(outputs.cpu().detach().numpy())\n",
    "                print(targets.cpu().detach().numpy())\n",
    "                plt.plot(targets.cpu().detach().numpy(), alpha=0.3)\n",
    "                plt.plot(outputs.cpu().detach().numpy())\n",
    "                plt.show()\n",
    "                r2 = r2_score(targets.cpu().detach().numpy(), outputs.cpu().detach().numpy())\n",
    "                return epoch_loss, lr, r2\n",
    "            else:\n",
    "                return epoch_loss, lr\n",
    "\n",
    "      \n",
    "        # define optimizer, scheduler and loss function\n",
    "        criterion = nn.MSELoss(reduction='sum')\n",
    "        optimizer = optim.AdamW(model_MO.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=scheduler_step_size, gamma=0.995)\n",
    "\n",
    "        # begin training\n",
    "        for epoch in range(n_epochs):\n",
    "            loss_train, lr_train = run_epoch(train_dataloader, is_training=True)\n",
    "            loss_val, lr_val, r2 = run_epoch(val_dataloader)\n",
    "            scheduler.step()\n",
    "\n",
    "            if save_path:\n",
    "                results = {\n",
    "                        'model': 'LSTM_MO',\n",
    "                        'pred_len': forecast_window,\n",
    "                        'epoch': epoch,\n",
    "                        'train_loss': loss_train,\n",
    "                        'val_loss': loss_val,\n",
    "                        'r2_val_sklearn': r2            \n",
    "                }\n",
    "\n",
    "                df = pd.DataFrame([results])\n",
    "                df.to_csv(save_path, mode='a', header=not os.path.exists(save_path), index=False)\n",
    "\n",
    "            \n",
    "            print('Epoch[{}/{}] | loss train:{:.6f}, val loss:{:.6f} | lr:{:.6f} | r2: {:.5f}|'\n",
    "                      .format(epoch+1, n_epochs, loss_train, loss_val, lr_train, r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkYm8LWuDd4-"
   },
   "source": [
    "## Defining the Many-to-Many LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T10:47:09.345962Z",
     "iopub.status.busy": "2024-10-15T10:47:09.345618Z",
     "iopub.status.idle": "2024-10-15T10:47:09.369066Z",
     "shell.execute_reply": "2024-10-15T10:47:09.368603Z",
     "shell.execute_reply.started": "2024-10-15T10:47:09.345944Z"
    },
    "id": "OzauwQ5VDd4_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class lstm_encoder(nn.Module):\n",
    "    ''' Encodes time-series sequence '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, dropout = 0.1, num_layers = 1):\n",
    "        super(lstm_encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
    "                            num_layers = num_layers, dropout = dropout)\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        lstm_out, self.hidden = self.lstm(x_input.view(x_input.shape[0], x_input.shape[1], self.input_size))\n",
    "        \n",
    "        return lstm_out, self.hidden     \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "\n",
    "\n",
    "class lstm_decoder(nn.Module):\n",
    "    ''' Decodes hidden state output by encoder '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, dropout = 0.1, num_layers = 1):\n",
    "        super(lstm_decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
    "                            num_layers = num_layers, dropout = dropout)\n",
    "        self.PReLU = nn.PReLU()\n",
    "        self.batchnorm = nn.BatchNorm1d(hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, input_size)           \n",
    "\n",
    "    def forward(self, x_input, encoder_hidden_states):\n",
    "        lstm_out, self.hidden = self.lstm(x_input.unsqueeze(0), encoder_hidden_states)\n",
    "        output = self.PReLU(lstm_out.squeeze(0))\n",
    "        #output = self.batchnorm(output)\n",
    "        output = self.linear(output)     \n",
    "        \n",
    "        return output, self.hidden\n",
    "\n",
    "class LSTM_MM(nn.Module):\n",
    "    ''' train LSTM encoder-decoder and make predictions '''\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(LSTM_MM, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.encoder = lstm_encoder(input_size = input_size, hidden_size = hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "        self.decoder = lstm_decoder(input_size = input_size, hidden_size = hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "\n",
    "\n",
    "    def train_model(self, input_tensor_train, target_tensor_train, input_tensor_val, target_tensor_val, n_epochs, target_len,\n",
    "                    batch_size,device, training_prediction = 'recursive', teacher_forcing_ratio = 0.5, learning_rate = 0.01, dynamic_tf = False):\n",
    "        '''\n",
    "        train lstm encoder-decoder\n",
    "        '''\n",
    "\n",
    "        # initialize array of losses \n",
    "        losses_train = np.full(n_epochs, np.nan)\n",
    "        losses_val = np.full(n_epochs, np.nan)\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # calculate number of batch iterations\n",
    "        n_batches_train = int(input_tensor_train.shape[1] / batch_size)\n",
    "        n_batches_val = int(input_tensor_val.shape[1] / batch_size)\n",
    "\n",
    "        with trange(n_epochs) as tr:\n",
    "            for it in tr:\n",
    "                self.train()\n",
    "                batch_loss = 0.\n",
    "\n",
    "                for b in range(n_batches_train):\n",
    "                    # select data \n",
    "                    input_batch = input_tensor_train[:, b: b + batch_size, :]\n",
    "                    target_batch = target_tensor_train[:, b: b + batch_size, :]\n",
    "                    # outputs tensor\n",
    "                    outputs = torch.zeros(target_len, batch_size, input_batch.shape[2])\n",
    "                    # initialize hidden state\n",
    "                    encoder_hidden = self.encoder.init_hidden(batch_size)\n",
    "                    # zero the gradient\n",
    "                    optimizer.zero_grad()\n",
    "                    # encoder outputs\n",
    "                    encoder_output, encoder_hidden = self.encoder(input_batch)\n",
    "                    # decoder with teacher forcing\n",
    "                    decoder_input = input_batch[-1, :, :]   # shape: (batch_size, input_size)\n",
    "                    decoder_hidden = encoder_hidden\n",
    "\n",
    "                    if training_prediction == 'recursive':\n",
    "                        # predict recursively\n",
    "                        for t in range(target_len): \n",
    "                            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                            outputs[t] = decoder_output\n",
    "                            decoder_input = decoder_output\n",
    "\n",
    "                    if training_prediction == 'teacher_forcing':\n",
    "                        # use teacher forcing\n",
    "                        if random.random() < teacher_forcing_ratio:\n",
    "                            for t in range(target_len): \n",
    "                                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                                outputs[t] = decoder_output\n",
    "                                decoder_input = target_batch[t, :, :]\n",
    "\n",
    "                        # predict recursively \n",
    "                        else:\n",
    "                            for t in range(target_len): \n",
    "                                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                                outputs[t] = decoder_output\n",
    "                                decoder_input = decoder_output\n",
    "\n",
    "                    if training_prediction == 'mixed_teacher_forcing':\n",
    "                        # predict using mixed teacher forcing\n",
    "                        for t in range(target_len):\n",
    "                            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                            outputs[t] = decoder_output\n",
    "                            \n",
    "                            # predict with teacher forcing\n",
    "                            if random.random() < teacher_forcing_ratio:\n",
    "                                decoder_input = target_batch[t, :, :]\n",
    "                            \n",
    "                            # predict recursively \n",
    "                            else:\n",
    "                                decoder_input = decoder_output\n",
    "\n",
    "                    # compute the loss \n",
    "                    loss = criterion(outputs.to(device), target_batch.to(device))\n",
    "                    batch_loss += loss.item()\n",
    "                    \n",
    "                    # backpropagation\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # loss for epoch \n",
    "                batch_loss /= n_batches_train\n",
    "                batch_loss = math.sqrt(batch_loss) \n",
    "                losses_train[it] = batch_loss\n",
    "\n",
    "                #validation\n",
    "                self.eval()                \n",
    "                batch_loss_val = 0.\n",
    "\n",
    "                for b in range(n_batches_val):\n",
    "                    # select data \n",
    "                    input_batch = input_tensor_val[:, b: b + batch_size, :]\n",
    "                    target_batch = target_tensor_val[:, b: b + batch_size, :]\n",
    "                    # outputs tensor\n",
    "                    outputs = torch.zeros(target_len, batch_size, input_batch.shape[2])\n",
    "                    # initialize hidden state\n",
    "                    encoder_hidden = self.encoder.init_hidden(batch_size)\n",
    "                    # encoder outputs\n",
    "                    encoder_output, encoder_hidden = self.encoder(input_batch)\n",
    "                    # decoder with teacher forcing\n",
    "                    decoder_input = input_batch[-1, :, :]   # shape: (batch_size, input_size)\n",
    "                    decoder_hidden = encoder_hidden\n",
    "\n",
    "                    if training_prediction == 'recursive':\n",
    "                        # predict recursively\n",
    "                        for t in range(target_len): \n",
    "                            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                            outputs[t] = decoder_output\n",
    "                            decoder_input = decoder_output\n",
    "\n",
    "                    if training_prediction == 'teacher_forcing':\n",
    "                        # use teacher forcing\n",
    "                        if random.random() < teacher_forcing_ratio:\n",
    "                            for t in range(target_len): \n",
    "                                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                                outputs[t] = decoder_output\n",
    "                                decoder_input = target_batch[t, :, :]\n",
    "\n",
    "                        # predict recursively \n",
    "                        else:\n",
    "                            for t in range(target_len): \n",
    "                                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                                outputs[t] = decoder_output\n",
    "                                decoder_input = decoder_output\n",
    "\n",
    "                    if training_prediction == 'mixed_teacher_forcing':\n",
    "                        # predict using mixed teacher forcing\n",
    "                        for t in range(target_len):\n",
    "                            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                            outputs[t] = decoder_output\n",
    "                            \n",
    "                            # predict with teacher forcing\n",
    "                            if random.random() < teacher_forcing_ratio:\n",
    "                                decoder_input = target_batch[t, :, :]\n",
    "                            \n",
    "                            # predict recursively \n",
    "                            else:\n",
    "                                decoder_input = decoder_output\n",
    "\n",
    "                    # compute the loss \n",
    "                    loss = criterion(outputs.to(device), target_batch.to(device))\n",
    "                    batch_loss_val += loss.item()\n",
    "                \n",
    "                                # loss for epoch \n",
    "                    batch_loss_val /= n_batches_val\n",
    "                    batch_loss_val = math.sqrt(batch_loss_val) \n",
    "                    losses_val[it] = batch_loss_val\n",
    "\n",
    "                # dynamic teacher forcing\n",
    "                if dynamic_tf and teacher_forcing_ratio > 0:\n",
    "                    teacher_forcing_ratio = teacher_forcing_ratio - 0.02\n",
    "                # progress bar \n",
    "                tr.set_postfix(train_loss=\"{0:.3f}\".format(batch_loss),val_loss=\"{0:.3f}\".format(batch_loss_val))\n",
    "                    \n",
    "        return losses_train, losses_val\n",
    "\n",
    "    def predict(self, input_tensor, target_len):\n",
    "        self.eval()\n",
    "\n",
    "        # encode input_tensor\n",
    "        input_tensor = input_tensor.unsqueeze(1)     # add in batch size of 1\n",
    "        encoder_output, encoder_hidden = self.encoder(input_tensor)\n",
    "\n",
    "        # initialize tensor for predictions\n",
    "        outputs = torch.zeros(target_len, input_tensor.shape[2])\n",
    "\n",
    "        # decode input_tensor\n",
    "        decoder_input = input_tensor[-1, :, :]\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for t in range(target_len):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            outputs[t] = decoder_output.squeeze(0)\n",
    "            decoder_input = decoder_output\n",
    "            \n",
    "        np_outputs = outputs.detach().numpy()\n",
    "        \n",
    "        return np_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66Rtd-wjSFcB"
   },
   "source": [
    "## Model training - Many to One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T10:47:09.370041Z",
     "iopub.status.busy": "2024-10-15T10:47:09.369864Z",
     "iopub.status.idle": "2024-10-15T10:47:09.373654Z",
     "shell.execute_reply": "2024-10-15T10:47:09.373204Z",
     "shell.execute_reply.started": "2024-10-15T10:47:09.370023Z"
    },
    "id": "KK88WJ6CLeUS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augment_trade_data(df, lag=0, forecast_window=None):\n",
    "    if forecast_window:\n",
    "        df['lag_return'] = np.log(df['price'].shift(forecast_window)/df['price'].shift(forecast_window+1))\n",
    "        return df.iloc[forecast_window+1:,:]\n",
    "    if lag == 0:\n",
    "        return df\n",
    "    else:\n",
    "        col_name = 'log_lag'+str(lag)+'_price'\n",
    "        df[col_name] = np.log(df.price) - np.log(df.price).shift(lag)\n",
    "        return df.iloc[lag:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2024-10-15T10:47:09.374551Z",
     "iopub.status.busy": "2024-10-15T10:47:09.374302Z"
    },
    "id": "pv-3a2LZJhN9",
    "outputId": "27b653f9-f01b-4cdb-8ea6-35f6ec083dc4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape (1375991, 10, 42) (1375991,)\n",
      "Validation data shape (343998, 10, 42) (343998,)\n"
     ]
    }
   ],
   "source": [
    "save_path = os.path.join('/home/gaen/Documents/codespace-gaen/Ts-master/playground_models/lstm_original',\n",
    "                            str(int(time.time()))+'_results.csv')\n",
    "\n",
    "forecast_windows = [i for i in range(1,31)]\n",
    "#forecast_windows = [1:31]\n",
    "\n",
    "for lag in forecast_windows:\n",
    "    orderbook = augment_trade_data(agg_trade, forecast_window=lag)\n",
    "\n",
    "    features = ['price', 'lag_return',\n",
    "                'bid1', 'bidqty1', 'bid2', 'bidqty2', 'bid3', 'bidqty3', 'bid4', 'bidqty4', 'bid5', 'bidqty5',\n",
    "                'bid6', 'bidqty6', 'bid7', 'bidqty7', 'bid8', 'bidqty8', 'bid9', 'bidqty9', 'bid10', 'bidqty10',\n",
    "                'ask1', 'askqty1', 'ask2', 'askqty2', 'ask3', 'askqty3', 'ask4', 'askqty4', 'ask5', 'askqty5',\n",
    "                'ask6', 'askqty6', 'ask7', 'askqty7', 'ask8', 'askqty8', 'ask9', 'askqty9', 'bid10', 'bidqty10']\n",
    "\n",
    "    split_index, data_x_train, data_y_train, data_x_val, data_y_val = prepare_data(np.array(orderbook[features][1_000_000:2_720_000]),\n",
    "                                                                                                                            np.array(agg_trade.datetime[2_005_000:2_006_000]),\n",
    "                                                                                                                            np.array(orderbook[features][60_000:60_600]),\n",
    "                                                                                                                            np.array(agg_trade.datetime[60_000:60_600]),\n",
    "                                                                                                                            config, lag=lag, plot=False)\n",
    "\n",
    "\n",
    "    dataset_train = TimeSeriesDataset(data_x_train, data_y_train)\n",
    "    dataset_val = TimeSeriesDataset(data_x_val, data_y_val)\n",
    "\n",
    "    print(\"Train data shape\", dataset_train.x.shape, dataset_train.y.shape)\n",
    "    print(\"Validation data shape\", dataset_val.x.shape, dataset_val.y.shape)\n",
    "    # print(\"Test data shape\", dataset_test.x.shape, dataset_test.y.shape)\n",
    "\n",
    "\n",
    "    model_MO = LSTM_MO(input_size=len(features), hidden_layer_size=64,\n",
    "                    num_layers=2, output_size=config[\"model_MO\"][\"output_size\"], dropout=0.2)\n",
    "\n",
    "    model_MO = model_MO.to(device)\n",
    "\n",
    "    # create DataLoader\n",
    "    train_dataloader = DataLoader(dataset_train, batch_size=config[\"training_MO\"][\"batch_size\"], shuffle=False)\n",
    "    val_dataloader = DataLoader(dataset_val, batch_size=config[\"training_MO\"][\"batch_size\"], shuffle=False)\n",
    "\n",
    "    model_MO.train_model(train_dataloader=train_dataloader, val_dataloader=val_dataloader, learning_rate=0.001,\n",
    "                scheduler_step_size=config[\"training_MO\"][\"scheduler_step_size\"], n_epochs=10,\n",
    "                device='cuda', save_path=save_path, forecast_window=lag)\n",
    "\n",
    "    # date_now = datetime.now()\n",
    "    # timestamp = date_now.strftime(\"%d-%b-%Y_%H:%M:%S.%f\")\n",
    "    del data_x_train \n",
    "    del data_y_train\n",
    "    del data_x_val\n",
    "    del data_y_val\n",
    "\n",
    "    torch.save(model_MO, f'/home/gaen/Documents/codespace-gaen/Ts-master/playground_models/lstm/No.1/LSTM_MO_LAG_{lag}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OG9B73Cxl1Ck",
    "outputId": "a8d4a5a6-5c9e-4b4d-9ab4-06511c5e8b49"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mhist(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_y_train)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m), bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(data_y_train, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(np.random.normal(size = len(data_y_train)*3), bins=150, alpha=0.5)\n",
    "plt.hist(data_y_train, bins=150, alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nk9uRrt44_mH",
    "outputId": "43366cd0-9e1f-4187-f2ba-843c5c34d895"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.087750500704166"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(data_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_7gEXQOyWAS",
    "outputId": "a83416ec-d2b9-4520-b749-8bbdb15f08f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(999900, 100, 2)\n",
      "(999900,)\n",
      "mean for lag 0 0.0\n",
      "std for lag 0 0.0\n",
      "(999899, 100, 2)\n",
      "(999899,)\n",
      "mean for lag 1 -0.002052977139672308\n",
      "std for lag 1 1.0858796465521126\n",
      "(999898, 100, 2)\n",
      "(999898,)\n",
      "mean for lag 2 -0.00410467584064107\n",
      "std for lag 2 1.556017011973598\n",
      "(999897, 100, 2)\n",
      "(999897,)\n",
      "mean for lag 3 -0.006154055526544967\n",
      "std for lag 3 1.9217677462946574\n",
      "(999896, 100, 2)\n",
      "(999896,)\n",
      "mean for lag 4 -0.008209040207585516\n",
      "std for lag 4 2.228366158793651\n",
      "(999895, 100, 2)\n",
      "(999895,)\n",
      "mean for lag 5 -0.010265174427752983\n",
      "std for lag 5 2.4953964400714703\n",
      "(999894, 100, 2)\n",
      "(999894,)\n",
      "mean for lag 6 -0.012325079093808363\n",
      "std for lag 6 2.7358125435737817\n",
      "(999893, 100, 2)\n",
      "(999893,)\n",
      "mean for lag 7 -0.014386081809158619\n",
      "std for lag 7 2.9571307584237307\n",
      "(999892, 100, 2)\n",
      "(999892,)\n",
      "mean for lag 8 -0.01644461902502651\n",
      "std for lag 8 3.162067860737714\n",
      "(999891, 100, 2)\n",
      "(999891,)\n",
      "mean for lag 9 -0.01850095315869017\n",
      "std for lag 9 3.3548281065515297\n",
      "(999890, 100, 2)\n",
      "(999890,)\n",
      "mean for lag 10 -0.020558902994898685\n",
      "std for lag 10 3.537155608454229\n",
      "(999889, 100, 2)\n",
      "(999889,)\n",
      "mean for lag 11 -0.022611658211680753\n",
      "std for lag 11 3.7120803017636343\n",
      "(999888, 100, 2)\n",
      "(999888,)\n",
      "mean for lag 12 -0.02466225354594861\n",
      "std for lag 12 3.8808702095312335\n",
      "(999887, 100, 2)\n",
      "(999887,)\n",
      "mean for lag 13 -0.026712854161459975\n",
      "std for lag 13 4.044880312115551\n",
      "(999886, 100, 2)\n",
      "(999886,)\n",
      "mean for lag 14 -0.028761323182285366\n",
      "std for lag 14 4.203159819962728\n",
      "(999885, 100, 2)\n",
      "(999885,)\n",
      "mean for lag 15 -0.030817527307400023\n",
      "std for lag 15 4.357240893877246\n",
      "(999884, 100, 2)\n",
      "(999884,)\n",
      "mean for lag 16 -0.03286600595474666\n",
      "std for lag 16 4.507623049042775\n",
      "(999883, 100, 2)\n",
      "(999883,)\n",
      "mean for lag 17 -0.034914500014565\n",
      "std for lag 17 4.653999189174414\n",
      "(999882, 100, 2)\n",
      "(999882,)\n",
      "mean for lag 18 -0.03696362944095826\n",
      "std for lag 18 4.796852126690666\n",
      "(999881, 100, 2)\n",
      "(999881,)\n",
      "mean for lag 19 -0.039013347223964344\n",
      "std for lag 19 4.935546171850454\n",
      "(999880, 100, 2)\n",
      "(999880,)\n",
      "mean for lag 20 -0.041069562932317324\n",
      "std for lag 20 5.070954087157907\n",
      "(999879, 100, 2)\n",
      "(999879,)\n",
      "mean for lag 21 -0.04311928849483244\n",
      "std for lag 21 5.202354852476439\n",
      "(999878, 100, 2)\n",
      "(999878,)\n",
      "mean for lag 22 -0.04517186550698742\n",
      "std for lag 22 5.331115430725617\n",
      "(999877, 100, 2)\n",
      "(999877,)\n",
      "mean for lag 23 -0.047224447891172756\n",
      "std for lag 23 5.455929523510874\n",
      "(999876, 100, 2)\n",
      "(999876,)\n",
      "mean for lag 24 -0.049272309827539995\n",
      "std for lag 24 5.577672557351427\n",
      "(999875, 100, 2)\n",
      "(999875,)\n",
      "mean for lag 25 -0.051326032187391304\n",
      "std for lag 25 5.697165151141915\n",
      "(999874, 100, 2)\n",
      "(999874,)\n",
      "mean for lag 26 -0.05337255142873986\n",
      "std for lag 26 5.814317438031195\n",
      "(999873, 100, 2)\n",
      "(999873,)\n",
      "mean for lag 27 -0.055418208250941334\n",
      "std for lag 27 5.928886728694055\n",
      "(999872, 100, 2)\n",
      "(999872,)\n",
      "mean for lag 28 -0.057462275570864675\n",
      "std for lag 28 6.041188956291376\n",
      "(999871, 100, 2)\n",
      "(999871,)\n",
      "mean for lag 29 -0.059506348149458925\n",
      "std for lag 29 6.151541924601213\n",
      "(999870, 100, 2)\n",
      "(999870,)\n",
      "mean for lag 30 -0.061556210241525876\n",
      "std for lag 30 6.259825384143825\n"
     ]
    }
   ],
   "source": [
    "lags = [i for i in range(31)]\n",
    "\n",
    "orderbook = augment_trade_data(agg_trade, forecast_window=lag)\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "for lag in lags:\n",
    "    split_index, data_x_train, data_y_train, data_x_val, data_y_val, data_x_unseen, data_x_test, data_y_test = prepare_data(np.array(orderbook[['price','bid1']][1_200_000:2_200_000]),\n",
    "                                                                                                                        np.array(agg_trade.datetime[2_000_000:2_100_000]),\n",
    "                                                                                                                        np.array(orderbook[['price','bid1']][60_000:65_000]),\n",
    "                                                                                                                        np.array(agg_trade.datetime[60_000:65_000]),\n",
    "                                                                                                                        config, lag=lag, plot=False)\n",
    "    mean = np.mean(data_y_train)\n",
    "    std =  np.std(data_y_train)\n",
    "    means.append(mean) \n",
    "    stds.append(std)             \n",
    "    print(f'mean for lag {lag} {mean}')\n",
    "    print(f'std for lag {lag} {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "mAr3rexp4WHn",
    "outputId": "30718f33-f01d-438f-cee3-c11f0fc1f2af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7b953036d0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "jNm_7cpy6GzX",
    "outputId": "57186ea5-0c7a-413d-a9b3-7a2447e53ea6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7b95ed3e90>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogrYY5L4BDX3",
    "outputId": "ff9b0b87-3156-4850-8c2b-a64d56d21c3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0858796465521126,\n",
       " 1.556017011973598,\n",
       " 1.9217677462946574,\n",
       " 2.228366158793651,\n",
       " 2.4953964400714703,\n",
       " 2.7358125435737817,\n",
       " 2.9571307584237307,\n",
       " 3.162067860737714,\n",
       " 3.3548281065515297,\n",
       " 3.537155608454229,\n",
       " 3.7120803017636343,\n",
       " 3.8808702095312335,\n",
       " 4.044880312115551,\n",
       " 4.203159819962728,\n",
       " 4.357240893877246,\n",
       " 4.507623049042775,\n",
       " 4.653999189174414,\n",
       " 4.796852126690666,\n",
       " 4.935546171850454,\n",
       " 5.070954087157907,\n",
       " 5.202354852476439,\n",
       " 5.331115430725617,\n",
       " 5.455929523510874,\n",
       " 5.577672557351427,\n",
       " 5.697165151141915,\n",
       " 5.814317438031195,\n",
       " 5.928886728694055,\n",
       " 6.041188956291376,\n",
       " 6.151541924601213,\n",
       " 6.259825384143825]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7OaAEH26Dd48",
    "gkYm8LWuDd4-",
    "9vX43xLNDd5A",
    "YQoZ0uHGSFcD",
    "h-X8WO0FSFcE"
   ],
   "machine_shape": "hm",
   "name": "3_predicting_ts_LSTM.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "lastestorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-15T15:17:12.868551Z",
     "iopub.status.busy": "2024-10-15T15:17:12.868054Z",
     "iopub.status.idle": "2024-10-15T15:17:18.103102Z",
     "shell.execute_reply": "2024-10-15T15:17:18.102347Z",
     "shell.execute_reply.started": "2024-10-15T15:17:12.868519Z"
    },
    "id": "KKMHFvHk8ubA",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m/mnt/workspace/shell\n"
     ]
    }
   ],
   "source": [
    "!pip --quiet install pytorch-warmup\n",
    "%cd /mnt/workspace/shell/\n",
    "\n",
    "#这个cd就是我家里主机上就这样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:15:55.053826Z",
     "iopub.status.busy": "2024-10-15T11:15:55.053454Z",
     "iopub.status.idle": "2024-10-15T11:16:00.094319Z",
     "shell.execute_reply": "2024-10-15T11:16:00.093687Z",
     "shell.execute_reply.started": "2024-10-15T11:15:55.053799Z"
    },
    "id": "orio-YPdlbsf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output \n",
    "!pip --quiet install pytorch_spiking pytorch_lightning #pytorch_forecasting \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-10-15T15:16:37.201927Z",
     "iopub.status.busy": "2024-10-15T15:16:37.201562Z",
     "iopub.status.idle": "2024-10-15T15:16:43.978544Z",
     "shell.execute_reply": "2024-10-15T15:16:43.978017Z",
     "shell.execute_reply.started": "2024-10-15T15:16:37.201904Z"
    },
    "id": "z68p_q4eISQP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "# import pickle\n",
    "import dill as pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import TransformerEncoder, TransformerEncoderLayer, LayerNorm\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_spiking\n",
    "import pytorch_warmup as warmup\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:16:13.025485Z",
     "iopub.status.busy": "2024-10-15T11:16:13.024992Z",
     "iopub.status.idle": "2024-10-15T11:16:13.031042Z",
     "shell.execute_reply": "2024-10-15T11:16:13.030512Z",
     "shell.execute_reply.started": "2024-10-15T11:16:13.025461Z"
    },
    "id": "FUcYIRwMIVPV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"plots\": {\n",
    "        \"show_plots\": False,\n",
    "        \"xticks_interval\": 1200,\n",
    "        \"color_actual\": \"#001f3f\",\n",
    "        \"color_train\": \"#3D9970\",\n",
    "        \"color_val\": \"#0074D9\",\n",
    "        \"color_test\": \"#FF4136\",\n",
    "        \"color_pred_train\": \"#3D9970\",\n",
    "        \"color_pred_val\": \"#0074D9\",\n",
    "        \"color_pred_test\": \"#FF4136\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"train_split_size\": 0.80,\n",
    "        \"input_window\": 30,\n",
    "        \"output_window\": 10,\n",
    "        \"train_batch_size\": 3,\n",
    "        \"eval_batch_size\": 1,\n",
    "        \"scaler\": \"normal\"\n",
    "    }, \n",
    "    \"model_transformer\": {\n",
    "        \"feature_size\": 250,\n",
    "        \"nhead\": 10,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.2,\n",
    "        \"out_features\": 1,\n",
    "        \"init_range\": 2, #0.5\n",
    "        \"lr\": 0.0002, #0.0001,\n",
    "        \"loss\": \"dilate\"\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"drive\": {\n",
    "            \"agg_trade\": {\n",
    "                \"train\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/input_data/\",\n",
    "                \"test\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/input_data/\", \n",
    "            },\n",
    "            \"orderbook\": {\n",
    "                \"train\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/input_data/\",\n",
    "                \"test\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/input_data/\",\n",
    "            },\n",
    "            \"models\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/models/\",\n",
    "            \"figures\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/figures/\",\n",
    "            \"utils\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/utils/\",\n",
    "        },\n",
    "        \n",
    "        \"local\": {\n",
    "            \"agg_trade\": {\n",
    "                \"train\": \"./data/input_data/\",\n",
    "                \"test\": \"./data/input_data/\", \n",
    "            },\n",
    "            \"orderbook\": {\n",
    "                \"train\": \"./data/input_data/\",\n",
    "                \"test\": \"./data/input_data/\",\n",
    "            },\n",
    "            \"models\": \"./models/\",\n",
    "            \"figures\": \"./figures/\",\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-15T11:16:54.024598Z",
     "iopub.status.busy": "2024-10-15T11:16:54.024211Z",
     "iopub.status.idle": "2024-10-15T11:16:54.028041Z",
     "shell.execute_reply": "2024-10-15T11:16:54.027469Z",
     "shell.execute_reply.started": "2024-10-15T11:16:54.024567Z"
    },
    "id": "uNNtAHKTjmDD",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "drive = False\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rxx4SuGIjmDE",
    "outputId": "d7848a6f-53b2-45ae-f722-26efe0efd5ea"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3H3a86TCSFb8"
   },
   "source": [
    "## Data preparation: augmenting raw financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:16:56.665337Z",
     "iopub.status.busy": "2024-10-15T11:16:56.664978Z",
     "iopub.status.idle": "2024-10-15T11:16:56.669577Z",
     "shell.execute_reply": "2024-10-15T11:16:56.669066Z",
     "shell.execute_reply.started": "2024-10-15T11:16:56.665314Z"
    },
    "id": "AGOE00q-ARLc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augment_trade_data(df, lag, forecast_window=None):\n",
    "    '''\n",
    "    Augmenting input data.\n",
    "    '''\n",
    "    if forecast_window:\n",
    "        df['lag_return'] = np.log(df['price'].shift(forecast_window)/df['price'].shift(forecast_window+1))\n",
    "        return df.iloc[forecast_window+1:,:]\n",
    "    if lag == 0:\n",
    "        return df\n",
    "    else:\n",
    "        col_name = 'log_lag'+str(lag)+'_price'\n",
    "        df[col_name] = np.log(df.price) - np.log(df.price).shift(lag)\n",
    "        return df.iloc[lag:,:]\n",
    "    \n",
    "#后续会用到 别急 就是模拟了一个正常交易的时候的延迟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4IH7QdtjmDH"
   },
   "source": [
    "## Defining Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on both ends of the time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))  # Permute for AvgPool1d (needs [batch, channel, seq_len])\n",
    "        x = x.permute(0, 2, 1)  # Revert back to [batch, seq_len, features]\n",
    "        return x\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "class DLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Decomposition-Linear (DLinear) model with output shape [batchsize, out_length].\n",
    "    \"\"\"\n",
    "    def __init__(self, configs):\n",
    "        super(DLinear, self).__init__()\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.pred_len = configs.pred_len\n",
    "  \n",
    "        # Decomposition Kernel Size\n",
    "        kernel_size = 25\n",
    "        self.decompsition = series_decomp(kernel_size)\n",
    "        self.individual = configs.individual\n",
    "        self.channels = configs.enc_in\n",
    "\n",
    "        if self.individual:\n",
    "            self.Linear_Seasonal = nn.ModuleList()\n",
    "            self.Linear_Trend = nn.ModuleList()\n",
    "            \n",
    "            for i in range(self.channels):\n",
    "                self.Linear_Seasonal.append(nn.Linear(self.seq_len, self.pred_len))\n",
    "                self.Linear_Trend.append(nn.Linear(self.seq_len, self.pred_len))\n",
    "        else:\n",
    "            self.Linear_Seasonal = nn.Linear(self.seq_len, self.pred_len)\n",
    "            self.Linear_Trend = nn.Linear(self.seq_len, self.pred_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: [Batch, Input length, Channels]\n",
    "        seasonal_init, trend_init = self.decompsition(x)\n",
    "        seasonal_init, trend_init = seasonal_init.permute(0, 2, 1), trend_init.permute(0, 2, 1)\n",
    "        \n",
    "        if self.individual:\n",
    "            seasonal_output = torch.zeros([seasonal_init.size(0), seasonal_init.size(1), self.pred_len], dtype=seasonal_init.dtype).to(seasonal_init.device)\n",
    "            trend_output = torch.zeros([trend_init.size(0), trend_init.size(1), self.pred_len], dtype=trend_init.dtype).to(trend_init.device)\n",
    "            for i in range(self.channels):\n",
    "                seasonal_output[:, i, :] = self.Linear_Seasonal[i](seasonal_init[:, i, :])\n",
    "                trend_output[:, i, :] = self.Linear_Trend[i](trend_init[:, i, :])\n",
    "        else:\n",
    "            seasonal_output = self.Linear_Seasonal(seasonal_init)  # [Batch, Channels, Pred_len]\n",
    "            trend_output = self.Linear_Trend(trend_init)  # [Batch, Channels, Pred_len]\n",
    "\n",
    "        # Sum of seasonal and trend components\n",
    "        output = seasonal_output + trend_output\n",
    "\n",
    "  \n",
    "        return output  # Output shape: [Batch, Pred_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFformer(nn.Module):\n",
    "    def __init__(self, n_time_series, seq_len, output_seq_len, d_model=128, n_heads=8,\n",
    "                 n_layers_encoder=6, dropout=0.1, output_dim=1, forward_dim=2048, use_mask=False, quantiles=None):\n",
    "        '''\n",
    "        Defining the HFformer model. Transformer encoder, linear decoder, with spiking activations.\n",
    "        '''\n",
    "        super(HFformer, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_time_series = n_time_series\n",
    "        self.d_model = d_model\n",
    "        self.nheads = n_heads\n",
    "        self.forward_dim = forward_dim\n",
    "        self.dropout = dropout\n",
    "        self.n_layers_encoder = n_layers_encoder\n",
    "        self.seq_len = seq_len\n",
    "        self.output_seq_len = output_seq_len\n",
    "        self.mask_it = use_mask\n",
    "        self.quantiles = quantiles\n",
    "        self.output_dim = output_dim \n",
    "\n",
    "        self.dense_shape = nn.Linear(self.n_time_series, self.d_model)\n",
    "        # spiking_activation = pytorch_spiking.SpikingActivation(nn.PReLU().to(self.device)).to(self.device)\n",
    "\n",
    "        self.encoder_layer = TransformerEncoderLayer(self.d_model, self.nheads, self.forward_dim, self.dropout)\n",
    "        self.encoder_norm = LayerNorm(self.d_model)\n",
    "        self.transformer_enc = TransformerEncoder(self.encoder_layer, self.n_layers_encoder, self.encoder_norm).to(self.device)        \n",
    "        self.output_dim_layer = nn.Linear(self.d_model, self.output_dim)\n",
    "        # self.output_dim_layer = nn.LSTM(self.d_model, self.output_dim, 1, batch_first=False)\n",
    "        if quantiles:\n",
    "            self.out_length_lay = nn.Linear(self.seq_len, len(quantiles))#, 1, batch_first=True)\n",
    "        else:\n",
    "            self.out_length_lay = nn.Linear(self.seq_len, self.output_seq_len)#, 1, batch_first=True)\n",
    "        self.mask = generate_square_subsequent_mask(self.seq_len).to(device)\n",
    "        self.activation = nn.PReLU() #pytorch_spiking.SpikingActivation(nn.PReLU().to('cuda')).to('cuda')\n",
    "        self.dlinear =  Model(seq_len = 1,\n",
    "            pred_len = 1,\n",
    "            enc_in = 38 , # number of input features\n",
    "            individual = True)\n",
    "\n",
    "    def make_embedding(self, x):\n",
    "        '''\n",
    "        Create embedding for model inputs.\n",
    "        '''\n",
    "        x = self.dense_shape(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        if self.mask_it:\n",
    "            x = self.transformer_enc(x, self.mask)\n",
    "        else:\n",
    "            x = self.transformer_enc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward method.\n",
    "        '''\n",
    "        x = self.dense_shape(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        if self.mask_it:\n",
    "            x = self.transformer_enc(x, self.mask)\n",
    "        else:\n",
    "            xiolk,m = self.transformer_enc(x)\n",
    "        \n",
    "        x = self\n",
    ".        x = self.output_dim_layer(x)\n",
    "        x = x.permute(1, 2, 0)\n",
    "        x = self.activation(x)\n",
    "        x = self.out_length_lay(x)\n",
    "        if self.output_dim > 1:\n",
    "            return x.permute(0, 2, 1)\n",
    "        if self.quantiles:\n",
    "            return x.view(-1, len(self.quantiles))\n",
    "        else:\n",
    "            return x.view(-1, self.output_seq_len)\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    '''\n",
    "    Generate mask.\n",
    "    '''\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "#模型的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN1amH-62AC7"
   },
   "source": [
    "## Defining Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:17:01.650362Z",
     "iopub.status.busy": "2024-10-15T11:17:01.650007Z",
     "iopub.status.idle": "2024-10-15T11:17:01.660153Z",
     "shell.execute_reply": "2024-10-15T11:17:01.659470Z",
     "shell.execute_reply.started": "2024-10-15T11:17:01.650339Z"
    },
    "id": "-ttkn5axAQCj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    '''\n",
    "    Class for converting LOB data into model inputs.\n",
    "    '''\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "\n",
    "\n",
    "def prepare_data_x(data, window_size, lag):\n",
    "    '''\n",
    "    Windows the input data for the ML models.\n",
    "    '''\n",
    "    n_row = data.shape[0] - window_size + 1\n",
    "    subset = data[:window_size]\n",
    "    subset_mean = np.mean(subset, axis=0)\n",
    "    output = np.zeros([n_row, window_size, len(subset_mean)])\n",
    "    x_mean = np.zeros([n_row, len(subset_mean)])\n",
    "    x_std = np.zeros([n_row, len(subset_mean)])\n",
    "    for idx in range(n_row):\n",
    "        subset = data[idx:idx+window_size]\n",
    "        subset_mean = np.mean(subset, axis=0)\n",
    "        subset_std = np.std(subset, axis=0) + 0.01\n",
    "        subset_norm = (subset-subset_mean)/subset_std\n",
    "        x_mean[idx,:] = subset_mean\n",
    "        x_std[idx,:] = subset_std\n",
    "        output[idx,:,:] = subset_norm\n",
    "    x_mean = np.array(x_mean)\n",
    "    x_std = np.array(x_std)\n",
    "    return output[:-lag-1], output[-1], x_mean, x_std\n",
    "\n",
    "\n",
    "def prepare_data_y(x, window_size, lag):\n",
    "    '''\n",
    "    Windows the target data for the ML models.\n",
    "    '''\n",
    "    output = np.zeros([len(x)-window_size-lag])\n",
    "    std = 1.1*np.sqrt(lag)+lag*0.01\n",
    "    for idx in range(0,len(x)-window_size-lag):\n",
    "        output[idx] = np.log(x[window_size+lag-1+idx,0]/x[window_size-1+idx,0])*10_000\n",
    "    output = output/std\n",
    "    return output\n",
    "\n",
    "\n",
    "def prepare_data(normalized_prices_train, dates_train, normalized_prices_test, dates_test, config, lag=1, plot=False):\n",
    "    '''\n",
    "    Returns input and target data.\n",
    "    '''\n",
    "    data_x, data_x_unseen, x_mean, x_std = prepare_data_x(normalized_prices_train, window_size=100, lag=lag)\n",
    "    data_y = prepare_data_y(normalized_prices_train, window_size=100, lag=lag)\n",
    "    split_index = int(data_y.shape[0]*0.8)\n",
    "    data_x_train = data_x[:split_index]\n",
    "    data_x_val = data_x[split_index:]\n",
    "    data_y_train = data_y[:split_index]\n",
    "    data_y_val = data_y[split_index:]\n",
    "\n",
    "    return split_index, data_x_train, data_y_train, data_x_val, data_y_val\n",
    "\n",
    "\n",
    "#这些就是来帮助把数据分成batch的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fydGNgID2Fsj"
   },
   "source": [
    "## Defining Custom Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:17:03.338090Z",
     "iopub.status.busy": "2024-10-15T11:17:03.337741Z",
     "iopub.status.idle": "2024-10-15T11:17:03.342106Z",
     "shell.execute_reply": "2024-10-15T11:17:03.341579Z",
     "shell.execute_reply.started": "2024-10-15T11:17:03.338068Z"
    },
    "id": "iH5GB6Lz507o",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantile_loss(y, y_pred, quantile):\n",
    "  '''\n",
    "  Computes quantile loss\n",
    "  Standard quantile loss as defined in the \"Training Procedure\" section of\n",
    "  the main TFT paper\n",
    "  '''\n",
    "  if quantile < 0 or quantile > 1:\n",
    "    raise ValueError(\n",
    "        'Illegal quantile value={}! Values should be between 0 and 1.'.format(\n",
    "            quantile))\n",
    "\n",
    "  prediction_underflow = y - y_pred\n",
    "  q_loss = quantile * torch.max(prediction_underflow, torch.zeros_like(prediction_underflow)) + (\n",
    "      1. - quantile) * torch.max(-prediction_underflow, torch.zeros_like(prediction_underflow))\n",
    "\n",
    "  return torch.sum(q_loss, axis=-1)\n",
    "\n",
    "\n",
    "#quantile loss在让模型有自信值很有帮助"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjErN2nj2NyY"
   },
   "source": [
    "## Defining Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:17:05.204011Z",
     "iopub.status.busy": "2024-10-15T11:17:05.203420Z",
     "iopub.status.idle": "2024-10-15T11:17:05.218381Z",
     "shell.execute_reply": "2024-10-15T11:17:05.217768Z",
     "shell.execute_reply.started": "2024-10-15T11:17:05.203987Z"
    },
    "id": "dDimPrK3SLZP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion_dict = {\"MAE\":nn.L1Loss, \"MSE\":nn.MSELoss, \"QuantileLoss\":quantile_loss}\n",
    "\n",
    "def compute_loss(labels, output, src, criterion):\n",
    "    '''\n",
    "    Computes loss\n",
    "    '''\n",
    "    print(labels.shape,output.shape)\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        if len(labels.shape) != len(output.shape):\n",
    "            if len(labels.shape) > 1:\n",
    "                if labels.shape[1] == output.shape[1]:\n",
    "                    labels = labels.unsqueeze(2)\n",
    "                else:\n",
    "                    labels = labels.unsqueeze(0)\n",
    "    loss = 0\n",
    "    loss = criterion(output, labels.float())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_step(model, opt, criterion, data_loader, takes_target, device,\n",
    "                       num_targets=1, forward_params={}):\n",
    "    '''\n",
    "    Performs training of a single model. Runs through one epoch of the data.\n",
    "    '''\n",
    "    i = 0\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for src, trg in data_loader:\n",
    "        opt.zero_grad()\n",
    "        if takes_target:\n",
    "            forward_params[\"t\"] = trg.to(device)\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        # Ensure all tensors in forward_params are on the correct device\n",
    "        # print(src.shape)\n",
    "        output = model(src,**forward_params)\n",
    "        output = output.squeeze()\n",
    "        if num_targets == 1:\n",
    "            labels = trg\n",
    "        elif num_targets > 1:\n",
    "            labels = trg[:, :, 0:num_targets]\n",
    "\n",
    "        \n",
    "        loss = compute_loss(labels, output, src, criterion[0])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "        i += 1\n",
    "    total_loss = running_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def validation(val_loader, model, criterion, device, num_targets=1):\n",
    "    '''\n",
    "    Computes the validation loss metrics.\n",
    "    '''\n",
    "    crit_losses = dict.fromkeys(criterion, 0)\n",
    "    model.eval()\n",
    "    labels = torch.Tensor(0).to(device)\n",
    "    labels_all = torch.Tensor(0).to(device)\n",
    "    output_all = torch.Tensor(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for src, targ in val_loader:\n",
    "            output = torch.Tensor(0).to(device)\n",
    "            src = src if isinstance(src, list) else src.to(device)\n",
    "            targ = targ if isinstance(targ, list) else targ.to(device)\n",
    "            output = model(src.float())\n",
    "            output = output.squeeze()\n",
    "            output_all = torch.cat((output_all, output))\n",
    "            if num_targets == 1:\n",
    "                labels = targ\n",
    "            elif num_targets > 1:\n",
    "                labels = targ[:, :, 0:num_targets]\n",
    "            for crit in criterion:\n",
    "                loss = compute_loss(labels, output, src, crit)\n",
    "                crit_losses[crit] += loss.item()\n",
    "            labels_all = torch.cat((labels_all, labels))\n",
    "    return list(crit_losses.values())[0], output_all, labels_all\n",
    "def forecast(data_loader, model, criterion, forecast_horizon, device, num_targets=1):\n",
    "    '''\n",
    "    Forecasting\n",
    "    '''\n",
    "    crit_losses = dict.fromkeys(criterion, 0)\n",
    "    model.eval()\n",
    "    output_decoder = torch.Tensor(0).to(device)\n",
    "    labels = torch.Tensor(0).to(device)\n",
    "    labels_all = torch.Tensor(0).to(device)\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for src, targ in data_loader:\n",
    "            if (counter % forecast_horizon) == 0:\n",
    "                src = src if isinstance(src, list) else src.to(device)\n",
    "                targ = targ if isinstance(targ, list) else targ.to(device)\n",
    "                output = model(src.float())\n",
    "                #output = output.reshape(1,-1)\n",
    "                output_decoder = torch.cat((output_decoder, output))\n",
    "                if num_targets == 1:\n",
    "                    labels = targ\n",
    "                elif num_targets > 1:\n",
    "                    labels = targ[:, :, 0:num_targets]\n",
    "                for crit in criterion:\n",
    "                    loss = compute_loss(labels, output, src, crit)\n",
    "                    crit_losses[crit] += loss.item()\n",
    "                labels_all = torch.cat((labels_all, labels))\n",
    "            counter += 1\n",
    "    return list(crit_losses.values())[0], output_decoder, labels_all\n",
    "\n",
    "\n",
    "#这些属于常规操作了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZOqAUud2UNQ"
   },
   "source": [
    "## Defining Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:17:07.215173Z",
     "iopub.status.busy": "2024-10-15T11:17:07.214814Z",
     "iopub.status.idle": "2024-10-15T11:17:07.220690Z",
     "shell.execute_reply": "2024-10-15T11:17:07.219919Z",
     "shell.execute_reply.started": "2024-10-15T11:17:07.215150Z"
    },
    "id": "uuQDDNHiFbf4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def strategy_evaluator(true, pred):\n",
    "    '''\n",
    "    Evaluates strategy regarding correct buys and sells\n",
    "    '''\n",
    "    total_buys, total_sells, total_holds = np.sum(true>0), np.sum(true<0), np.sum(true==0)\n",
    "    total_correct_buys, total_correct_sells, total_correct_holds = 0, 0, 0\n",
    "    for idx in range(len(true)):\n",
    "        for jdx in range(len(true[0])):\n",
    "            if true[idx,jdx] > 0 and pred[idx,jdx] > 0:\n",
    "                total_correct_buys += 1\n",
    "            elif true[idx,jdx] < 0 and pred[idx,jdx] < 0:\n",
    "                total_correct_sells += 1\n",
    "            elif true[idx,jdx] == 0 and pred[idx,jdx] == 0:\n",
    "                total_correct_holds += 1\n",
    "    total_correct_buys_r, total_correct_sells_r, total_correct_holds_r = (total_correct_buys/total_buys),(total_correct_sells/total_sells),(total_correct_holds/total_holds)\n",
    "    return total_correct_buys_r.round(3), total_correct_sells_r.round(3), total_correct_holds_r.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:17:09.072073Z",
     "iopub.status.busy": "2024-10-15T11:17:09.071694Z",
     "iopub.status.idle": "2024-10-15T11:17:09.083317Z",
     "shell.execute_reply": "2024-10-15T11:17:09.082774Z",
     "shell.execute_reply.started": "2024-10-15T11:17:09.072050Z"
    },
    "id": "mUp74e95PcwZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainer(model, train_loader, validation_loader, test_loader, criterion, opt, scheduler,\n",
    "            warmup_scheduler, max_epochs, batch_size, forecast_horizon, takes_target, shuffle=False,\n",
    "            num_targets=1, plot_prediction=True, save_path='/mnt/workspace/shell/results_Transencwithlineardec'\n",
    ", LAG=0):\n",
    "    '''\n",
    "    Training method\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_loader = DataLoader(train_loader, batch_size=batch_size, shuffle=shuffle, sampler=None, batch_sampler=None, num_workers=10)\n",
    "    validation_data_loader = DataLoader(validation_loader, batch_size=batch_size, shuffle=False, sampler=None, batch_sampler=None, num_workers=10)\n",
    "    test_data_loader = DataLoader(test_loader, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=2)\n",
    "    forecast_data_loader = DataLoader(validation_loader, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=2)\n",
    "    \n",
    "    for epoch in range(1, max_epochs+1):\n",
    "\n",
    "        total_loss = train_step(model, opt, criterion, data_loader, takes_target, device, num_targets=num_targets)\n",
    "        val_loss = 0\n",
    "        if plot_prediction:\n",
    "            val_loss, val_values, true_values = forecast(forecast_data_loader, model, criterion, forecast_horizon=forecast_horizon,\n",
    "                                                                   device=device, num_targets=num_targets)\n",
    "            fig, ax = plt.subplots(1, 1, figsize = (18, 8))\n",
    "            ax.plot(true_values.cpu().view(-1), label='truth', alpha=0.3)\n",
    "            ax.plot(val_values.cpu().view(-1), label='forecast', alpha=0.8)\n",
    "            ax.set_xlim(left=0, right=len(true_values.cpu().view(-1)))\n",
    "            plt.show()\n",
    "        else:\n",
    "            val_loss, val_values, true_values = validation(validation_data_loader, model, criterion, device,\n",
    "                                                            num_targets=num_targets)\n",
    "            # val_loss, val_values, true_values, src_all\n",
    "        preds, trues = val_values.cpu().numpy(), true_values.cpu().numpy()#, src_all.cpu().numpy()\n",
    "\n",
    "        print(f'preds {preds.shape}')\n",
    "        print(f'trues {trues.shape}')\n",
    "\n",
    "        results = 0\n",
    "      \n",
    "        r2_sklearn = r2_score(trues, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print('-' * 88)\n",
    "        print('| epoch {:3d} | {:5.2f} s | train loss {:5.5f} | val loss {:5.5f} | lr {:1.8f} | r2 sklearn: {:1.5f} | b, s, h: {:}|'.format(\n",
    "                        epoch, elapsed, total_loss, val_loss, scheduler.get_last_lr()[0], r2_sklearn, results))\n",
    "        print('-' * 88)\n",
    "        start_time = time.time()\n",
    "\n",
    "        if save_path:\n",
    "            results = {\n",
    "                    'model': 'Dlinear',\n",
    "                    'pred_len': forecast_horizon,\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss': total_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'r2_val_sklearn': r2_sklearn            \n",
    "            }\n",
    "\n",
    "            df = pd.DataFrame([results])\n",
    "            df.to_csv(os.path.join(save_path, 'results.csv'), mode='a', header=not os.path.exists(save_path), index=False)\n",
    "            save_directory = os.path.join(save_path, \"s4\")\n",
    "            if not os.path.exists(save_directory):\n",
    "                os.makedirs(save_directory)\n",
    "            if r2_sklearn >0.02 :\n",
    "                torch.save(model.state_dict(), os.path.join(save_path,\"s4\",f'_epoch_{epoch}_time_{time.time()}_r2_{r2_sklearn}.pt'))\n",
    "\n",
    "        with warmup_scheduler.dampening():\n",
    "            scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJ5zrDoD2X1k"
   },
   "source": [
    "## Model and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mq_67bISiIAb"
   },
   "source": [
    "## Optimal paramater search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:17:25.182203Z",
     "iopub.status.busy": "2024-10-15T11:17:25.181837Z",
     "iopub.status.idle": "2024-10-15T11:17:57.412219Z",
     "shell.execute_reply": "2024-10-15T11:17:57.411611Z",
     "shell.execute_reply.started": "2024-10-15T11:17:25.182173Z"
    },
    "id": "rq97xGiXkzPs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# date_train = 'all' \n",
    "# date_test = 'all'\n",
    "date_train = 'All_to_Sept'\n",
    "date_test = 'All_to_Sept'\n",
    "\n",
    "idx = 0\n",
    "agg_trade = pd.read_csv(config[\"paths\"][\"local\"][\"agg_trade\"][\"train\"]+date_train+'/orderbook_agg_trade_dollarvol.csv')\n",
    "agg_trade['w_midprice'] = (agg_trade['ask1']*agg_trade['askqty1']+agg_trade['bid1']*agg_trade['bidqty1'])/(agg_trade['askqty1']+agg_trade['bidqty1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T14:04:16.129433Z",
     "iopub.status.busy": "2024-10-15T14:04:16.129073Z",
     "iopub.status.idle": "2024-10-15T14:04:16.331286Z",
     "shell.execute_reply": "2024-10-15T14:04:16.330629Z",
     "shell.execute_reply.started": "2024-10-15T14:04:16.129408Z"
    },
    "id": "1oD9yQb9g-Ov",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/software/anaconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/mnt/software/anaconda3/envs/python3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 256, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x256 and 100x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/workspace/shell/7_Dlinear.ipynb Cell 24\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m scheduler \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mExponentialLR(optimizer, gamma\u001b[39m=\u001b[39m\u001b[39m0.98\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m warmup_scheduler \u001b[39m=\u001b[39m warmup\u001b[39m.\u001b[39mLinearWarmup(optimizer, warmup_period\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m trainer(model_custom, train_loader, val_loader, test_loader, [criterion], optimizer, scheduler, warmup_scheduler, epochs, batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m     forecast_horizon\u001b[39m=\u001b[39;49mforecast_window, takes_target\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, plot_prediction\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, save_path\u001b[39m=\u001b[39;49msave_path, LAG\u001b[39m=\u001b[39;49mforecast_window)\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mdel\u001b[39;00m data_x_train \n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mdel\u001b[39;00m data_y_train\n",
      "\u001b[1;32m/mnt/workspace/shell/7_Dlinear.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m forecast_data_loader \u001b[39m=\u001b[39m DataLoader(validation_loader, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, sampler\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, batch_sampler\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     total_loss \u001b[39m=\u001b[39m train_step(model, opt, criterion, data_loader, takes_target, device, num_targets\u001b[39m=\u001b[39;49mnum_targets)\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mif\u001b[39;00m plot_prediction:\n",
      "\u001b[1;32m/mnt/workspace/shell/7_Dlinear.ipynb Cell 24\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m trg \u001b[39m=\u001b[39m trg\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Ensure all tensors in forward_params are on the correct device\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# print(src.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m output \u001b[39m=\u001b[39m model(src,\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mif\u001b[39;00m num_targets \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/mnt/software/anaconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/software/anaconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/mnt/workspace/shell/7_Dlinear.ipynb Cell 24\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m     xiolk,m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_enc(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdlinear(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_dim_layer(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/mnt/software/anaconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/software/anaconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/mnt/workspace/shell/7_Dlinear.ipynb Cell 24\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m     trend_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros([trend_init\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), trend_init\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpred_len], dtype\u001b[39m=\u001b[39mtrend_init\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mto(trend_init\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannels):\n\u001b[0;32m---> <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m         seasonal_output[:, i, :] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLinear_Seasonal[i](seasonal_init[:, i, :])\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m         trend_output[:, i, :] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLinear_Trend[i](trend_init[:, i, :])\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-shanghai.data.aliyun.com/mnt/workspace/shell/7_Dlinear.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/software/anaconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/software/anaconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/software/anaconda3/envs/python3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x256 and 100x256)"
     ]
    }
   ],
   "source": [
    "model_name = 'Dlinear_tmp_1_withtransformer'\n",
    "\n",
    "save_path = os.path.join(f'/mnt/workspace/shell/tmp/{model_name}/training_details/Dlinear/results_Dlinear',\n",
    "                            str(int(time.time()))+'_results.csv')\n",
    "\n",
    "# save_path = f'/mnt/workspace/shell/tmp/{model_name}/training_details/HFTransformer/results_HFformer'\n",
    "# # save_path=None\n",
    "# save_path = f'/home/gaen/Documents/codespace-gaen/Simons/{model_name}/training_details/HFTransformer/results_HFformer'\n",
    "filepath = f'/mnt/workspace/shell/tmp/{model_name}/training_details/s4/results_Dlinear/Dlinear'\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "os.makedirs(filepath, exist_ok=True)\n",
    "\n",
    "forecast_history = 100\n",
    "epochs = 32\n",
    "batch_size = 256 #64 for linear decoder\n",
    "\n",
    "# forecast_windows = [i for i in range(1,32)]\n",
    "forecast_windows = [5, 8, 10, 13, 14, 15, 16, 24, 25, 26, 28, 29, 30]\n",
    "for forecast_window in forecast_windows:\n",
    "    \n",
    "    # orderbook = augment_trade_data(agg_trade, lag=0, forecast_window=forecast_window)\n",
    "    orderbook = augment_trade_data(agg_trade, lag=0, forecast_window=forecast_window)\n",
    "    # total_orw = orderbook.shape[0]\n",
    "    # print(total_orw)\n",
    "    features = ['price', 'lag_return',\n",
    "                'bid1', 'bidqty1', 'bid2', 'bidqty2', 'bid3', 'bidqty3', 'bid4', 'bidqty4', 'bid5', 'bidqty5',\n",
    "                'bid6', 'bidqty6', 'bid7', 'bidqty7', 'bid8', 'bidqty8', 'bid9', 'bidqty9',\n",
    "                'ask1', 'askqty1', 'ask2', 'askqty2', 'ask3', 'askqty3', 'ask4', 'askqty4', 'ask5', 'askqty5',\n",
    "                'ask6', 'askqty6', 'ask7', 'askqty7', 'ask8', 'askqty8', 'ask9', 'askqty9']\n",
    "\n",
    "\n",
    "    split_index, data_x_train, data_y_train, data_x_val, data_y_val = prepare_data(np.array(orderbook[features][1_350_000:1_350_500]),\n",
    "                                                                                                                            np.array(agg_trade.datetime[899_999:1_000_000]),\n",
    "                                                                                                                            np.array(orderbook[features][60_000:60_600]),\n",
    "                                                                                                                            np.array(agg_trade.datetime[60_000:60_600]),\n",
    "                                                                                                                            config, lag=forecast_window, plot=False)\n",
    "\n",
    "    # data_x_train_shape = data_x_train.shape[1]\n",
    "    # data_y_train_shape = data_y_train.shape[1]\n",
    "    # data_x_val_shape = data_x_val.shape[1]\n",
    "    # data_y_val_shape = data_y_val.shape[1]\n",
    "    # print( data_x_train_shape , data_y_train_shape ,data_x_val_shape, data_y_val_shape)\n",
    "    train_loader = TimeSeriesDataset(data_x_train, data_y_train)\n",
    "    val_loader = TimeSeriesDataset(data_x_val, data_y_val)\n",
    "    test_loader = None\n",
    "\n",
    "    model_custom = HFformer(n_time_series=len(features), seq_len=forecast_history, output_seq_len=1, d_model=64,\n",
    "                  n_heads=8, n_layers_encoder=2, dropout=0.3, output_dim=1, forward_dim=64, use_mask=True).to(device)\n",
    "\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "    optimizer = optim.AdamW(model_custom.parameters(), lr=0.1, amsgrad=True)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "    warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period=1000)\n",
    "\n",
    "    trainer(model_custom, train_loader, val_loader, test_loader, [criterion], optimizer, scheduler, warmup_scheduler, epochs, batch_size=batch_size,\n",
    "        forecast_horizon=forecast_window, takes_target=False, plot_prediction=False, save_path=save_path, LAG=forecast_window)\n",
    "    \n",
    "    del data_x_train \n",
    "    del data_y_train\n",
    "    del data_x_val\n",
    "    del data_y_val\n",
    "\n",
    "state = {\n",
    "    'model': model.state_dict(),\n",
    "    'epoch': forecast_window,\n",
    "}\n",
    "torch.save(state, f'/mnt/workspace/shell/tmp/{model_name}/Dlinear/Dlinear_forecasting_FINAL_horizon_{forecast_window}.pt')\n",
    "    # torch.save(model_custom, f'/mnt/workspace/shell/tmp/{model_name}/s4_enclinear_forecasting_FINAL_horizon_{forecast_window}.pt')\n",
    "print(f'Done with prediction len {forecast_window}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    date_train = 'all' \n",
    "    date_test = 'all'\n",
    "    drive = None\n",
    "    if drive:\n",
    "        agg_trade = pd.read_csv(config[\"paths\"][\"drive\"][\"agg_trade\"][\"train\"]+date_train+'/orderbook.csv')    \n",
    "        sys.path.append(config[\"paths\"][\"drive\"][\"utils\"])\n",
    "    else:\n",
    "        agg_trade = pd.read_csv(config[\"paths\"][\"local\"][\"agg_trade\"][\"train\"]+date_train+'/orderbook_agg_trade_dollarvol_drop_near_duplicate_price.csv')\n",
    "        agg_trade_test = pd.read_csv(config[\"paths\"][\"local\"][\"agg_trade\"][\"test\"]+date_test+'/orderbook_agg_trade_dollarvol_drop_near_duplicate_price.csv')\n",
    "    idx = 0\n",
    "    agg_trade['w_midprice'] = (agg_trade['ask1']*agg_trade['askqty1']+agg_trade['bid1']*agg_trade['bidqty1'])/(agg_trade['askqty1']+agg_trade['bidqty1'])\n",
    "\n",
    "    #agg_trade['price'] = agg_trade['w_midprice']\n",
    "    agg_trade_test = agg_trade[4_500_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    save_path = os.path.join('./home/gaen/Documents/codespace-gaen/Simons/models_drop_duplicate_near/training_details/HFTransformer/results_HFformer',\n",
    "                                str(int(time.time()))+'_results.csv')\n",
    "\n",
    "    save_path = '/home/gaen/Documents/codespace-gaen/Simons/models_drop_duplicate_near/training_details/HFTransformer/results_HFformer'\n",
    "    # save_path=None\n",
    "\n",
    "    forecast_history = 100 \n",
    "    epochs = 16\n",
    "    batch_size = 300 #64 for linear decoder\n",
    "\n",
    "    forecast_windows = [i for i in range(1,31)]\n",
    "\n",
    "    for forecast_window in forecast_windows:\n",
    "        \n",
    "        orderbook = augment_trade_data(agg_trade, lag=0, forecast_window=forecast_window)\n",
    "\n",
    "        features = ['price', 'lag_return',\n",
    "                    'bid1', 'bidqty1', 'bid2', 'bidqty2', 'bid3', 'bidqty3', 'bid4', 'bidqty4', 'bid5', 'bidqty5',\n",
    "                    'bid6', 'bidqty6', 'bid7', 'bidqty7', 'bid8', 'bidqty8', 'bid9', 'bidqty9',\n",
    "                    'ask1', 'askqty1', 'ask2', 'askqty2', 'ask3', 'askqty3', 'ask4', 'askqty4', 'ask5', 'askqty5',\n",
    "                    'ask6', 'askqty6', 'ask7', 'askqty7', 'ask8', 'askqty8', 'ask9', 'askqty9']\n",
    "\n",
    "        split_index, data_x_train, data_y_train, data_x_val, data_y_val = prepare_data(np.array(orderbook[features][1_000_000-700000:1_720_000-700000]),\n",
    "                                                                                                                                np.array(agg_trade.datetime[2_005_000-1200000:2_006_000-1200000]),\n",
    "                                                                                                                                np.array(orderbook[features][60_000:60_600]),\n",
    "                                                                                                                                np.array(agg_trade.datetime[60_000:60_600]),\n",
    "                                                                                                                                config, lag=forecast_window, plot=False)\n",
    "\n",
    "\n",
    "        train_loader = TimeSeriesDataset(data_x_train, data_y_train)\n",
    "        val_loader = TimeSeriesDataset(data_x_val, data_y_val)\n",
    "        test_loader = None\n",
    "\n",
    "        model_custom = HFformer(n_time_series=len(features), seq_len=forecast_history, output_seq_len=1, d_model=36,\n",
    "                    n_heads=6, n_layers_encoder=2, dropout=0.3, output_dim=1, forward_dim=64, use_mask=True).to(device)\n",
    "\n",
    "        criterion = nn.MSELoss(reduction='sum')\n",
    "        optimizer = optim.AdamW(model_custom.parameters(), lr=0.1, amsgrad=True)\n",
    "        scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "        warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period=1000)\n",
    "\n",
    "        trainer(model_custom, train_loader, val_loader, test_loader, [criterion], optimizer, scheduler, warmup_scheduler, epochs, batch_size=batch_size,\n",
    "            forecast_horizon=forecast_window, takes_target=False, plot_prediction=False, save_path=save_path, LAG=forecast_window)\n",
    "        \n",
    "        del data_x_train \n",
    "        del data_y_train\n",
    "        del data_x_val\n",
    "        del data_y_val\n",
    "\n",
    "        torch.save(model_custom, f'./models_drop_duplicate_near/transformer_enclinear_forecasting_FINAL_horizon_{forecast_window}.pt')\n",
    "        print(f'Done with prediction len {forecast_window}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRVCqRHngARx"
   },
   "source": [
    "## Forecast Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwYi2KUoQ61u"
   },
   "outputs": [],
   "source": [
    "def strategy_evaluator(true, pred, weighted=False):\n",
    "    '''\n",
    "    Evaluates trading strategy based on correct buys and sells.\n",
    "    '''\n",
    "    total_buys, total_sells, total_holds = np.sum(true>0), np.sum(true<0), np.sum(true==0)\n",
    "    total_correct_buys, total_correct_sells, total_correct_holds = 0, 0, 0\n",
    "    for idx in range(len(true)):\n",
    "        if true[idx] > 0 and pred[idx] > 0:\n",
    "            total_correct_buys += 1\n",
    "        elif true[idx] < 0 and pred[idx] < 0:\n",
    "            total_correct_sells += 1\n",
    "        elif true[idx] == 0 and pred[idx] == 0:\n",
    "            total_correct_holds += 1\n",
    "    total_correct_buys_r, total_correct_sells_r, total_correct_holds_r = (total_correct_buys/total_buys),(total_correct_sells/total_sells),(total_correct_holds/total_holds)\n",
    "    total_correct_trades = (total_correct_buys+total_correct_sells+total_correct_holds)/(total_buys+total_sells+total_holds)\n",
    "    return total_buys, total_correct_buys, total_sells, total_correct_sells, total_holds, total_correct_holds\n",
    "\n",
    "\n",
    "def forecast_evaluator(test_loader, model, criterion, forecast_horizon=1, device=device, num_targets=1, save_path=None):\n",
    "    '''\n",
    "    Outputs evaluation metrics.\n",
    "    '''\n",
    "    test_data_loader = DataLoader(test_loader, batch_size=128, shuffle=False, sampler=None, batch_sampler=None, num_workers=6)\n",
    "    loss, pred, true = forecast(test_data_loader, model, criterion, forecast_horizon=1, device=device, num_targets=1)\n",
    "    pred, true = pred.cpu().numpy(), true.cpu().numpy()\n",
    "\n",
    "    r2 = r2_score(true, pred)\n",
    "    strategy_results = strategy_evaluator(true, pred)\n",
    "    \n",
    "    if save_path:\n",
    "        results = {\n",
    "                'model': 'Transencwithlineardec',\n",
    "                'pred_len': forecast_horizon,\n",
    "                'test_loss': loss,\n",
    "                'r2_val_sklearn': r2,\n",
    "                'correct_buys': strategy_results[1],\n",
    "                'total_buys':  strategy_results[0],\n",
    "                'correct_sells': strategy_results[3],\n",
    "                'total_sells': strategy_results[2],\n",
    "                'correct_holds': strategy_results[5],\n",
    "                'total_holds': strategy_results[4],\n",
    "        }\n",
    "        os.makedirs(save_path_model, exist_ok=True)\n",
    "      \n",
    "        df.to_csv(save_path, mode='a', header=not os.path.exists(save_path), index=False)\n",
    "\n",
    "    print(f'| test loss {loss} | b, cb, s, cs, h, ch: {strategy_results} |')\n",
    "\n",
    "    return pred, true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djC9UZA6gARy"
   },
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UeQr6KHN1HDZ"
   },
   "outputs": [],
   "source": [
    "date_train = 'all' \n",
    "date_test = 'all'\n",
    "drive  = None\n",
    "if drive:\n",
    "    agg_trade = pd.read_csv(config[\"paths\"][\"drive\"][\"agg_trade\"][\"train\"]+date_train+'/orderbook.csv')    \n",
    "    sys.path.append(config[\"paths\"][\"drive\"][\"utils\"])\n",
    "else:\n",
    "    agg_trade = pd.read_csv(config[\"paths\"][\"local\"][\"agg_trade\"][\"train\"]+date_train+'/orderbook_agg_trade_dollarvol.csv')\n",
    "    agg_trade_test = pd.read_csv(config[\"paths\"][\"local\"][\"agg_trade\"][\"test\"]+date_test+'/orderbook_agg_trade_dollarvol.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_trade_test = agg_trade[1_200_000:]\n",
    "print(agg_trade_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-vAZLp5gARy"
   },
   "outputs": [],
   "source": [
    "save_path = os.path.join('/mnt/workspace/shell/models/training_details/HFTransformer/results_HFformer_tmp',\n",
    "                            str(int(time.time()))+'_forecasting_results.csv')\n",
    "\n",
    "# save_path = None\n",
    "\n",
    "save_path_results = os.path.join('/mnt/workspace/shell/models/training_details/HFTransformer/results_HFformer_tmp',\n",
    "                            str(int(time.time()))+'_list_results.pkl')\n",
    "\n",
    "save_path_model = os.path.join('/mnt/workspace/shell/models/training_details/HFTransformer/results_HFformer_tmp',str(int(time.time()))+'_model.pth')\n",
    "\n",
    "forecast_history = 100 \n",
    "batch_size = 256 #64 for linear decoder\n",
    "\n",
    "forecast_windows = [i for i in range(1,32)]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for forecast_window in forecast_windows:\n",
    "    \n",
    "    orderbook = augment_trade_data(agg_trade_test, lag=0, forecast_window=forecast_window)\n",
    "\n",
    "    features = ['price', 'lag_return',\n",
    "                'bid1', 'bidqty1', 'bid2', 'bidqty2', 'bid3', 'bidqty3', 'bid4', 'bidqty4', 'bid5', 'bidqty5',\n",
    "                'bid6', 'bidqty6', 'bid7', 'bidqty7', 'bid8', 'bidqty8', 'bid9', 'bidqty9',\n",
    "                'ask1', 'askqty1', 'ask2', 'askqty2', 'ask3', 'askqty3', 'ask4', 'askqty4', 'ask5', 'askqty5',\n",
    "                'ask6', 'askqty6', 'ask7', 'askqty7', 'ask8', 'askqty8', 'ask9', 'askqty9']\n",
    "    print(orderbook[features].shape)\n",
    "\n",
    "    split_index, data_x_train, data_y_train, data_x_val, data_y_val = prepare_data(np.array(orderbook[features][:]),\n",
    "                                                                                    np.array(agg_trade.datetime[2_005_000-500000:2_006_00-5000000]),\n",
    "                                                                                    np.array(orderbook[features][60_000:60_600]),\n",
    "                                                                                    np.array(agg_trade.datetime[60_000:60_600]),\n",
    "                                                                                    config, lag=forecast_window, plot=True)\n",
    "\n",
    "    test_loader = TimeSeriesDataset(data_x_train, data_y_train)\n",
    "\n",
    "    model = torch.load(f'/mnt/workspace/shell/tmp/HFfMODELsept10test_tmp_1/transformer_enclinear_forecasting_FINAL_horizon_{forecast_window}.pt')\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    pred, true = forecast_evaluator(test_loader, model, [criterion], forecast_horizon=forecast_window, device=device, num_targets=1, save_path=save_path)\n",
    "    \n",
    "    predictions.append((pred, true))\n",
    "\n",
    "    print(f'Done with prediction len {forecast_window}.')\n",
    "\n",
    "with open(save_path_results, 'wb') as f:\n",
    "    pickle.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "7_HFformerv2.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

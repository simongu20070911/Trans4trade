{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-15T15:17:12.868551Z",
     "iopub.status.busy": "2024-10-15T15:17:12.868054Z",
     "iopub.status.idle": "2024-10-15T15:17:18.103102Z",
     "shell.execute_reply": "2024-10-15T15:17:18.102347Z",
     "shell.execute_reply.started": "2024-10-15T15:17:12.868519Z"
    },
    "id": "KKMHFvHk8ubA",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gaen/Documents/codespace-gaen/Ts-master\n"
     ]
    }
   ],
   "source": [
    "%cd /home/gaen/Documents/codespace-gaen/Ts-master\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:15:55.053826Z",
     "iopub.status.busy": "2024-10-15T11:15:55.053454Z",
     "iopub.status.idle": "2024-10-15T11:16:00.094319Z",
     "shell.execute_reply": "2024-10-15T11:16:00.093687Z",
     "shell.execute_reply.started": "2024-10-15T11:15:55.053799Z"
    },
    "id": "orio-YPdlbsf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2024-10-15T15:16:37.201927Z",
     "iopub.status.busy": "2024-10-15T15:16:37.201562Z",
     "iopub.status.idle": "2024-10-15T15:16:43.978544Z",
     "shell.execute_reply": "2024-10-15T15:16:43.978017Z",
     "shell.execute_reply.started": "2024-10-15T15:16:37.201904Z"
    },
    "id": "z68p_q4eISQP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules import TransformerEncoder, TransformerEncoderLayer, LayerNorm\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_spiking\n",
    "import pytorch_warmup as warmup\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) not found. Install by going to extensions/kernels/ and running `python setup.py install`, for improved speed and memory efficiency. Note that the kernel changed for state-spaces 4.0 and must be recompiled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KeOps] Warning : Cuda libraries were not detected on the system or could not be loaded ; using cpu only mode\n"
     ]
    }
   ],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from models_s4.s4.s4 import S4Block as S4  # Can use full version instead of minimal S4D standalone below\n",
    "from models_s4.s4.s4d import S4D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:16:13.025485Z",
     "iopub.status.busy": "2024-10-15T11:16:13.024992Z",
     "iopub.status.idle": "2024-10-15T11:16:13.031042Z",
     "shell.execute_reply": "2024-10-15T11:16:13.030512Z",
     "shell.execute_reply.started": "2024-10-15T11:16:13.025461Z"
    },
    "id": "FUcYIRwMIVPV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"plots\": {\n",
    "        \"show_plots\": False,\n",
    "        \"xticks_interval\": 1200,\n",
    "        \"color_actual\": \"#001f3f\",\n",
    "        \"color_train\": \"#3D9970\",\n",
    "        \"color_val\": \"#0074D9\",\n",
    "        \"color_test\": \"#FF4136\",\n",
    "        \"color_pred_train\": \"#3D9970\",\n",
    "        \"color_pred_val\": \"#0074D9\",\n",
    "        \"color_pred_test\": \"#FF4136\",\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"train_split_size\": 0.80,\n",
    "        \"input_window\": 30,\n",
    "        \"output_window\": 10,\n",
    "        \"train_batch_size\": 3,\n",
    "        \"eval_batch_size\": 1,\n",
    "        \"scaler\": \"normal\"\n",
    "    }, \n",
    "    \"model_transformer\": {\n",
    "        \"feature_size\": 250,\n",
    "        \"nhead\": 10,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.2,\n",
    "        \"out_features\": 1,\n",
    "        \"init_range\": 2, #0.5\n",
    "        \"lr\": 0.0002, #0.0001,\n",
    "        \"loss\": \"dilate\"\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"drive\": {\n",
    "            \"agg_trade\": {\n",
    "                \"train\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/input_data/\",\n",
    "                \"test\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/input_data/\", \n",
    "            },\n",
    "            \"orderbook\": {\n",
    "                \"train\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/input_data/\",\n",
    "                \"test\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/input_data/\",\n",
    "            },\n",
    "            \"models\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/models/\",\n",
    "            \"figures\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/figures/\",\n",
    "            \"utils\": \"/content/drive/MyDrive/IP/Repos/HFTransformer/utils/\",\n",
    "        },\n",
    "        \n",
    "        \"local\": {\n",
    "            \"agg_trade\": {\n",
    "                \"train\": \"./data/input_data/\",\n",
    "                \"test\": \"./data/input_data/\", \n",
    "            },\n",
    "            \"orderbook\": {\n",
    "                \"train\": \"./data/input_data/\",\n",
    "                \"test\": \"./data/input_data/\",\n",
    "            },\n",
    "            \"models\": \"./models/\",\n",
    "            \"figures\": \"./figures/\",\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-10-15T11:16:54.024598Z",
     "iopub.status.busy": "2024-10-15T11:16:54.024211Z",
     "iopub.status.idle": "2024-10-15T11:16:54.028041Z",
     "shell.execute_reply": "2024-10-15T11:16:54.027469Z",
     "shell.execute_reply.started": "2024-10-15T11:16:54.024567Z"
    },
    "id": "uNNtAHKTjmDD",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "drive = False\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rxx4SuGIjmDE",
    "outputId": "d7848a6f-53b2-45ae-f722-26efe0efd5ea"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3H3a86TCSFb8"
   },
   "source": [
    "## Data preparation: augmenting raw financial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:16:56.665337Z",
     "iopub.status.busy": "2024-10-15T11:16:56.664978Z",
     "iopub.status.idle": "2024-10-15T11:16:56.669577Z",
     "shell.execute_reply": "2024-10-15T11:16:56.669066Z",
     "shell.execute_reply.started": "2024-10-15T11:16:56.665314Z"
    },
    "id": "AGOE00q-ARLc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augment_trade_data(df, lag, forecast_window=None):\n",
    "    '''\n",
    "    Augmenting input data.\n",
    "    '''\n",
    "    if forecast_window:\n",
    "        df['lag_return'] = np.log(df['price'].shift(forecast_window)/df['price'].shift(forecast_window+1))\n",
    "        return df.iloc[forecast_window+1:,:]\n",
    "    if lag == 0:\n",
    "        return df\n",
    "    else:\n",
    "        col_name = 'log_lag'+str(lag)+'_price'\n",
    "        df[col_name] = np.log(df.price) - np.log(df.price).shift(lag)\n",
    "        return df.iloc[lag:,:]\n",
    "    \n",
    "#后续会用到 别急 就是模拟了一个正常交易的时候的延迟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4IH7QdtjmDH"
   },
   "source": [
    "## Defining Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tuple(map(int, torch.__version__.split('.')[:2])) == (1, 11):\n",
    "    print(\"WARNING: Dropout is bugged in PyTorch 1.11. Results may be worse.\")\n",
    "    dropout_fn = nn.Dropout\n",
    "if tuple(map(int, torch.__version__.split('.')[:2])) >= (1, 12):\n",
    "    dropout_fn = nn.Dropout1d\n",
    "else:\n",
    "    dropout_fn = nn.Dropout2d\n",
    "\n",
    "\n",
    "class S4Model(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_input,\n",
    "        d_output=10,\n",
    "        d_model=256,\n",
    "        n_layers=4,\n",
    "        dropout=0.2,\n",
    "        prenorm=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.prenorm = prenorm\n",
    "        self.encoder = nn.Linear(d_input, d_model)\n",
    "        # Stack S4 layers as residual blocks\n",
    "        self.s4_layers = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        for _ in range(n_layers):\n",
    "            self.s4_layers.append(\n",
    "                S4D(d_model, dropout=dropout, transposed=True, lr=min(0.001, 0.01))\n",
    "            )\n",
    "            self.norms.append(nn.LayerNorm(d_model))\n",
    "            self.dropouts.append(dropout_fn(dropout))\n",
    "        # Linear decoder\n",
    "        self.decoder = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input x is shape (B, L, d_input)\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)  # (B, L, d_input) -> (B, L, d_model)\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN detected in 10101010 output\")\n",
    "        x = x.transpose(-1, -2)  # (B, L, d_model) -> (B, d_model, L)\n",
    "        for layer, norm, dropout in zip(self.s4_layers, self.norms, self.dropouts):\n",
    "            # Each iteration of this loop will map (B, d_model, L) -> (B, d_model, L)\n",
    "\n",
    "            z = x\n",
    "            if self.prenorm:\n",
    "                # Prenorm\n",
    "                z = norm(z.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "            # Apply S4 block: we ignore the state input and output\n",
    "            z, _ = layer(z)\n",
    "\n",
    "            # Dropout on the output of the S4 block\n",
    "            z = dropout(z)\n",
    "\n",
    "            # Residual connection\n",
    "            x = z + x\n",
    "\n",
    "            if not self.prenorm:\n",
    "                # Postnorm\n",
    "                x = norm(x.transpose(-1, -2)).transpose(-1, -2)\n",
    "\n",
    "        x = x.transpose(-1, -2)\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN detected in -1 output\")\n",
    "        x = self.decoder(x)  # (B, d_model) -> (B, d_output)\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN detected in 0 output\")\n",
    "        return x\n",
    "# torch.Size([256, 64, 100])\n",
    "# torch.Size([100, 256, 36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead), \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.transformer_encoder(x)\n",
    "\n",
    "\n",
    "class S4Transformer(nn.Module):\n",
    "    def __init__(self, input_size, d_model=36, nhead=4, num_layers=3, output_size=1):\n",
    "        super(S4Transformer, self).__init__()\n",
    "        self.s4 = S4Model(\n",
    "                d_input=38,#len(features),\n",
    "                d_output=1,\n",
    "                d_model=36,\n",
    "                n_layers=3,\n",
    "                dropout=0.1,\n",
    "                prenorm=\"store_true\")\n",
    "        self.transformer_encoder = TransformerEncoderLayer(d_model, nhead, num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: [batch_size, seq_len, input_size]\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN detected in 1 output\")\n",
    "        x = self.s4(x)\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN detected in 2 output\")\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer_encoder(x)\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN detected in 3 output\")\n",
    "        x = x.permute(1, 0, 2)\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN detected in 4 output\")\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN detected in 5 output\")\n",
    "        x = self.fc_out(x)\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN detected in 6 output\")\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN1amH-62AC7"
   },
   "source": [
    "## Defining Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:17:01.650362Z",
     "iopub.status.busy": "2024-10-15T11:17:01.650007Z",
     "iopub.status.idle": "2024-10-15T11:17:01.660153Z",
     "shell.execute_reply": "2024-10-15T11:17:01.659470Z",
     "shell.execute_reply.started": "2024-10-15T11:17:01.650339Z"
    },
    "id": "-ttkn5axAQCj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    '''\n",
    "    Class for converting LOB data into model inputs.\n",
    "    '''\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.x[idx], self.y[idx])\n",
    "\n",
    "\n",
    "def prepare_data_x(data, window_size, lag):\n",
    "    '''\n",
    "    Windows the input data for the ML models.\n",
    "    '''\n",
    "    n_row = data.shape[0] - window_size + 1\n",
    "    subset = data[:window_size]\n",
    "    subset_mean = np.mean(subset, axis=0)\n",
    "    output = np.zeros([n_row, window_size, len(subset_mean)])\n",
    "    x_mean = np.zeros([n_row, len(subset_mean)])\n",
    "    x_std = np.zeros([n_row, len(subset_mean)])\n",
    "    for idx in range(n_row):\n",
    "        subset = data[idx:idx+window_size]\n",
    "        subset_mean = np.mean(subset, axis=0)\n",
    "        subset_std = np.std(subset, axis=0) + 0.01\n",
    "        subset_norm = (subset-subset_mean)/subset_std\n",
    "        x_mean[idx,:] = subset_mean\n",
    "        x_std[idx,:] = subset_std\n",
    "        output[idx,:,:] = subset_norm\n",
    "    x_mean = np.array(x_mean)\n",
    "    x_std = np.array(x_std)\n",
    "    return output[:-lag-1], output[-1], x_mean, x_std\n",
    "\n",
    "\n",
    "def prepare_data_y(x, window_size, lag):\n",
    "    '''\n",
    "    Windows the target data for the ML models.\n",
    "    '''\n",
    "    output = np.zeros([len(x)-window_size-lag])\n",
    "    std = 1.1*np.sqrt(lag)+lag*0.01\n",
    "    for idx in range(0,len(x)-window_size-lag):\n",
    "        output[idx] = np.log(x[window_size+lag-1+idx,0]/x[window_size-1+idx,0])*10_000\n",
    "    output = output/std\n",
    "    return output\n",
    "\n",
    "\n",
    "def prepare_data(normalized_prices_train, dates_train, normalized_prices_test, dates_test, config, lag=1, plot=False):\n",
    "    '''\n",
    "    Returns input and target data.\n",
    "    '''\n",
    "    data_x, data_x_unseen, x_mean, x_std = prepare_data_x(normalized_prices_train, window_size=100, lag=lag)\n",
    "    data_y = prepare_data_y(normalized_prices_train, window_size=100, lag=lag)\n",
    "    split_index = int(data_y.shape[0]*0.8)\n",
    "    data_x_train = data_x[:split_index]\n",
    "    data_x_val = data_x[split_index:]\n",
    "    data_y_train = data_y[:split_index]\n",
    "    data_y_val = data_y[split_index:]\n",
    "\n",
    "    return split_index, data_x_train, data_y_train, data_x_val, data_y_val\n",
    "\n",
    "\n",
    "#这些就是来帮助把数据分成batch的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fydGNgID2Fsj"
   },
   "source": [
    "## Defining Custom Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:17:03.338090Z",
     "iopub.status.busy": "2024-10-15T11:17:03.337741Z",
     "iopub.status.idle": "2024-10-15T11:17:03.342106Z",
     "shell.execute_reply": "2024-10-15T11:17:03.341579Z",
     "shell.execute_reply.started": "2024-10-15T11:17:03.338068Z"
    },
    "id": "iH5GB6Lz507o",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantile_loss(y, y_pred, quantile):\n",
    "  '''\n",
    "  Computes quantile loss\n",
    "  Standard quantile loss as defined in the \"Training Procedure\" section of\n",
    "  the main TFT paper\n",
    "  '''\n",
    "  if quantile < 0 or quantile > 1:\n",
    "    raise ValueError(\n",
    "        'Illegal quantile value={}! Values should be between 0 and 1.'.format(\n",
    "            quantile))\n",
    "\n",
    "  prediction_underflow = y - y_pred\n",
    "  q_loss = quantile * torch.max(prediction_underflow, torch.zeros_like(prediction_underflow)) + (\n",
    "      1. - quantile) * torch.max(-prediction_underflow, torch.zeros_like(prediction_underflow))\n",
    "\n",
    "  return torch.sum(q_loss, axis=-1)\n",
    "\n",
    "\n",
    "#quantile loss在让模型有自信值很有帮助"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjErN2nj2NyY"
   },
   "source": [
    "## Defining Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:17:05.204011Z",
     "iopub.status.busy": "2024-10-15T11:17:05.203420Z",
     "iopub.status.idle": "2024-10-15T11:17:05.218381Z",
     "shell.execute_reply": "2024-10-15T11:17:05.217768Z",
     "shell.execute_reply.started": "2024-10-15T11:17:05.203987Z"
    },
    "id": "dDimPrK3SLZP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion_dict = {\"MAE\":nn.L1Loss, \"MSE\":nn.MSELoss, \"QuantileLoss\":quantile_loss}\n",
    "\n",
    "def compute_loss(labels, output, src, criterion):\n",
    "    '''\n",
    "    Computes loss\n",
    "    '''\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        if len(labels.shape) != len(output.shape):\n",
    "            if len(labels.shape) > 1:\n",
    "                if labels.shape[1] == output.shape[1]:\n",
    "                    labels = labels.unsqueeze(2)\n",
    "                else:\n",
    "                    labels = labels.unsqueeze(0)\n",
    "    loss = 0\n",
    "    loss = criterion(output, labels.float())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_step(model, opt, criterion, data_loader, takes_target, device,\n",
    "                       num_targets=1, forward_params={}):\n",
    "    '''\n",
    "    Performs training of a single model. Runs through one epoch of the data.\n",
    "    '''\n",
    "    i = 0\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    for src, trg in data_loader:\n",
    "        opt.zero_grad()\n",
    "        if takes_target:\n",
    "            forward_params[\"t\"] = trg.to(device)\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        # Ensure all tensors in forward_params are on the correct device\n",
    "        # print(src.shape)\n",
    "        output = model(src,**forward_params)\n",
    "        output = output.squeeze()\n",
    "        if num_targets == 1:\n",
    "            labels = trg\n",
    "        elif num_targets > 1:\n",
    "            labels = trg[:, :, 0:num_targets]\n",
    "\n",
    "        \n",
    "        loss = compute_loss(labels, output, src, criterion[0])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "        i += 1\n",
    "    total_loss = running_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def validation(val_loader, model, criterion, device, num_targets=1):\n",
    "    '''\n",
    "    Computes the validation loss metrics.\n",
    "    '''\n",
    "    crit_losses = dict.fromkeys(criterion, 0)\n",
    "    model.eval()\n",
    "    labels = torch.Tensor(0).to(device)\n",
    "    labels_all = torch.Tensor(0).to(device)\n",
    "    output_all = torch.Tensor(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for src, targ in val_loader:\n",
    "            output = torch.Tensor(0).to(device)\n",
    "            src = src if isinstance(src, list) else src.to(device)\n",
    "            targ = targ if isinstance(targ, list) else targ.to(device)\n",
    "            output = model(src.float())\n",
    "            output = output.squeeze()\n",
    "            output_all = torch.cat((output_all, output))\n",
    "            if num_targets == 1:\n",
    "                labels = targ\n",
    "            elif num_targets > 1:\n",
    "                labels = targ[:, :, 0:num_targets]\n",
    "            for crit in criterion:\n",
    "                loss = compute_loss(labels, output, src, crit)\n",
    "                crit_losses[crit] += loss.item()\n",
    "            labels_all = torch.cat((labels_all, labels))\n",
    "    return list(crit_losses.values())[0], output_all, labels_all\n",
    "def forecast(data_loader, model, criterion, forecast_horizon, device, num_targets=1):\n",
    "    '''\n",
    "    Forecasting\n",
    "    '''\n",
    "    crit_losses = dict.fromkeys(criterion, 0)\n",
    "    model.eval()\n",
    "    output_decoder = torch.Tensor(0).to(device)\n",
    "    labels = torch.Tensor(0).to(device)\n",
    "    labels_all = torch.Tensor(0).to(device)\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for src, targ in data_loader:\n",
    "            if (counter % forecast_horizon) == 0:\n",
    "                src = src if isinstance(src, list) else src.to(device)\n",
    "                targ = targ if isinstance(targ, list) else targ.to(device)\n",
    "                output = model(src.float())\n",
    "                #output = output.reshape(1,-1)\n",
    "                output_decoder = torch.cat((output_decoder, output))\n",
    "                if num_targets == 1:\n",
    "                    labels = targ\n",
    "                elif num_targets > 1:\n",
    "                    labels = targ[:, :, 0:num_targets]\n",
    "                for crit in criterion:\n",
    "                    loss = compute_loss(labels, output, src, crit)\n",
    "                    crit_losses[crit] += loss.item()\n",
    "                labels_all = torch.cat((labels_all, labels))\n",
    "            counter += 1\n",
    "    return list(crit_losses.values())[0], output_decoder, labels_all\n",
    "\n",
    "\n",
    "#这些属于常规操作了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZOqAUud2UNQ"
   },
   "source": [
    "## Defining Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:17:07.215173Z",
     "iopub.status.busy": "2024-10-15T11:17:07.214814Z",
     "iopub.status.idle": "2024-10-15T11:17:07.220690Z",
     "shell.execute_reply": "2024-10-15T11:17:07.219919Z",
     "shell.execute_reply.started": "2024-10-15T11:17:07.215150Z"
    },
    "id": "uuQDDNHiFbf4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def strategy_evaluator(true, pred):\n",
    "    '''\n",
    "    Evaluates strategy regarding correct buys and sells\n",
    "    '''\n",
    "    total_buys, total_sells, total_holds = np.sum(true>0), np.sum(true<0), np.sum(true==0)\n",
    "    total_correct_buys, total_correct_sells, total_correct_holds = 0, 0, 0\n",
    "    for idx in range(len(true)):\n",
    "        for jdx in range(len(true[0])):\n",
    "            if true[idx,jdx] > 0 and pred[idx,jdx] > 0:\n",
    "                total_correct_buys += 1\n",
    "            elif true[idx,jdx] < 0 and pred[idx,jdx] < 0:\n",
    "                total_correct_sells += 1\n",
    "            elif true[idx,jdx] == 0 and pred[idx,jdx] == 0:\n",
    "                total_correct_holds += 1\n",
    "    total_correct_buys_r, total_correct_sells_r, total_correct_holds_r = (total_correct_buys/total_buys),(total_correct_sells/total_sells),(total_correct_holds/total_holds)\n",
    "    return total_correct_buys_r.round(3), total_correct_sells_r.round(3), total_correct_holds_r.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:17:09.072073Z",
     "iopub.status.busy": "2024-10-15T11:17:09.071694Z",
     "iopub.status.idle": "2024-10-15T11:17:09.083317Z",
     "shell.execute_reply": "2024-10-15T11:17:09.082774Z",
     "shell.execute_reply.started": "2024-10-15T11:17:09.072050Z"
    },
    "id": "mUp74e95PcwZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainer(model, train_loader, validation_loader, test_loader, criterion, opt, scheduler,\n",
    "            warmup_scheduler, max_epochs, batch_size, forecast_horizon, takes_target, shuffle=False,\n",
    "            num_targets=1, plot_prediction=True, save_path='/mnt/workspace/shell/results_Transencwithlineardec'\n",
    ", LAG=0):\n",
    "    '''\n",
    "    Training method\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_loader = DataLoader(train_loader, batch_size=batch_size, shuffle=False, sampler=None, batch_sampler=None, num_workers=10)\n",
    "    validation_data_loader = DataLoader(validation_loader, batch_size=batch_size, shuffle=False, sampler=None, batch_sampler=None, num_workers=10)\n",
    "    test_data_loader = DataLoader(test_loader, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=2)\n",
    "    forecast_data_loader = DataLoader(validation_loader, batch_size=1, shuffle=False, sampler=None, batch_sampler=None, num_workers=2)\n",
    "    \n",
    "    for epoch in range(1, max_epochs+1):\n",
    "\n",
    "        total_loss = train_step(model, opt, criterion, data_loader, takes_target, device, num_targets=num_targets)\n",
    "        val_loss = 0\n",
    "        if plot_prediction:\n",
    "            val_loss, val_values, true_values = forecast(forecast_data_loader, model, criterion, forecast_horizon=forecast_horizon,\n",
    "                                                                   device=device, num_targets=num_targets)\n",
    "            fig, ax = plt.subplots(1, 1, figsize = (18, 8))\n",
    "            ax.plot(true_values.cpu().view(-1), label='truth', alpha=0.3)\n",
    "            ax.plot(val_values.cpu().view(-1), label='forecast', alpha=0.8)\n",
    "            ax.set_xlim(left=0, right=len(true_values.cpu().view(-1)))\n",
    "            plt.show()\n",
    "        else:\n",
    "            val_loss, val_values, true_values = validation(validation_data_loader, model, criterion, device,\n",
    "                                                            num_targets=num_targets)\n",
    "            # val_loss, val_values, true_values, src_all\n",
    "        preds, trues = val_values.cpu().numpy(), true_values.cpu().numpy()#, src_all.cpu().numpy()\n",
    "\n",
    "\n",
    "        # print(f'preds {preds.shape}')\n",
    "        # print(f'trues {trues.shape}')\n",
    "\n",
    "        results = 0\n",
    "    \n",
    "            \n",
    "        r2_sklearn = r2_score(trues, preds)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print('-' * 88)\n",
    "        print('| epoch {:3d} | {:5.2f} s | train loss {:5.5f} | val loss {:5.5f} | lr {:1.8f} | r2 sklearn: {:1.5f} | b, s, h: {:}|'.format(\n",
    "                        epoch, elapsed, total_loss, val_loss, scheduler.get_last_lr()[0], r2_sklearn, results))\n",
    "        print('-' * 88)\n",
    "        start_time = time.time()\n",
    "\n",
    "        if save_path:\n",
    "            results = {\n",
    "                    'model': 'Transencwithlineardec',\n",
    "                    'pred_len': forecast_horizon,\n",
    "                    'epoch': epoch,\n",
    "                    'train_loss': total_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                    'r2_val_sklearn': r2_sklearn            \n",
    "            }\n",
    "\n",
    "            df = pd.DataFrame([results])\n",
    "            df.to_csv(os.path.join(save_path, 'results.csv'), mode='a', header=not os.path.exists(save_path), index=False)\n",
    "            save_directory = os.path.join(save_path, \"Transencwithlineardec\")\n",
    "            if not os.path.exists(save_directory):\n",
    "                os.makedirs(save_directory)\n",
    "            if r2_sklearn >0.02 :\n",
    "                torch.save(model.state_dict(), os.path.join(save_path,\"Transencwithlineardec\",f'_epoch_{epoch}_time_{time.time()}_r2_{r2_sklearn}.pt'))\n",
    "\n",
    "        with warmup_scheduler.dampening():\n",
    "            scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJ5zrDoD2X1k"
   },
   "source": [
    "## Model and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mq_67bISiIAb"
   },
   "source": [
    "## Optimal paramater search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T11:17:25.182203Z",
     "iopub.status.busy": "2024-10-15T11:17:25.181837Z",
     "iopub.status.idle": "2024-10-15T11:17:57.412219Z",
     "shell.execute_reply": "2024-10-15T11:17:57.411611Z",
     "shell.execute_reply.started": "2024-10-15T11:17:25.182173Z"
    },
    "id": "rq97xGiXkzPs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# date_train = 'all' \n",
    "# date_test = 'all'\n",
    "date_train = 'All_to_Sept'\n",
    "date_test = 'All_to_Sept'\n",
    "\n",
    "\n",
    "# drive = None\n",
    "# if False:\n",
    "#     if drive:\n",
    "#         agg_trade = pd.read_csv(config[\"paths\"][\"drive\"][\"agg_trade\"][\"train\"]+date_train+'/orderbook.csv')    \n",
    "#         sys.path.append(config[\"paths\"][\"drive\"][\"utils\"])\n",
    "#     else: \n",
    "#         agg_trade = pd.read_csv(config[\"paths\"][\"local\"][\"agg_trade\"][\"train\"]+date_train+'/orderbook_agg_trade_dollarvol_drop_duplicate_price.csv')\n",
    "#         agg_trade_test = pd.read_csv(config[\"paths\"][\"local\"][\"agg_trade\"][\"test\"]+date_test+'/orderbook_agg_trade_dollarvol_drop_duplicate_price.csv')\n",
    "idx = 0\n",
    "agg_trade = pd.read_csv(config[\"paths\"][\"local\"][\"agg_trade\"][\"train\"]+date_train+'/orderbook_agg_trade_dollarvol.csv')\n",
    "# agg_trade_test = pd.read_csv(config[\"paths\"][\"local\"][\"agg_trade\"][\"test\"]+date_test+'/orderbook_agg_trade_dollarvol.csv')\n",
    "agg_trade['w_midprice'] = (agg_trade['ask1']*agg_trade['askqty1']+agg_trade['bid1']*agg_trade['bidqty1'])/(agg_trade['askqty1']+agg_trade['bidqty1'])\n",
    "# total_rows = agg_trade.shape[0]\n",
    "# total_vols = agg_trade.shape[1]\n",
    "# #agg_trade['price'] = agg_trade['w_midprice']\n",
    "# # agg_trade_test = agg_trade[4_500_000:]\n",
    "# print(total_rows,total_vols)\n",
    "# orderbook = augment_trade_data(agg_trade, lag=0, forecast_window=100)\n",
    "# features = ['price', 'lag_return',\n",
    "#                 'bid1', 'bidqty1', 'bid2', 'bidqty2', 'bid3', 'bidqty3', 'bid4', 'bidqty4', 'bid5', 'bidqty5',\n",
    "#                 'bid6', 'bidqty6', 'bid7', 'bidqty7', 'bid8', 'bidqty8', 'bid9', 'bidqty9',\n",
    "#                 'ask1', 'askqty1', 'ask2', 'askqty2', 'ask3', 'askqty3', 'ask4', 'askqty4', 'ask5', 'askqty5',\n",
    "#                 'ask6', 'askqty6', 'ask7', 'askqty7', 'ask8', 'askqty8', 'ask9', 'askqty9']\n",
    "        \n",
    "# data_array =  np.array(orderbook[features][1_000_000:1_720_000])\n",
    "# data_array =  np.array(orderbook[features][:])\n",
    "# # 查看 shape\n",
    "# print(data_array.shape)\n",
    "# # 打印前 10 行（索引 0 到 9）\n",
    "# print(\"前 10 行:\")\n",
    "# print(data_array[0:10])\n",
    "\n",
    "# # 打印索引从 1,000,000 到 1,000,009 的数据\n",
    "# # 注意：确保 data_array 的长度足够大，否则可能会引发索引错误\n",
    "# print(\"\\n索引从 1,000,000 到 1,000,009 的数据:\")\n",
    "# print(data_array[719_990:720_000])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-15T14:04:16.129433Z",
     "iopub.status.busy": "2024-10-15T14:04:16.129073Z",
     "iopub.status.idle": "2024-10-15T14:04:16.331286Z",
     "shell.execute_reply": "2024-10-15T14:04:16.330629Z",
     "shell.execute_reply.started": "2024-10-15T14:04:16.129408Z"
    },
    "id": "1oD9yQb9g-Ov",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaen/miniconda3/envs/lastestorch/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "| epoch   1 | 26.52 s | train loss 34877.59388 | val loss 1429.58628 | lr 0.10000000 | r2 sklearn: -0.00796 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   2 | 26.31 s | train loss 34636.73940 | val loss 1408.06999 | lr 0.09800000 | r2 sklearn: 0.00721 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   3 | 26.27 s | train loss 34551.05287 | val loss 1376.73766 | lr 0.09604000 | r2 sklearn: 0.02930 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   4 | 26.32 s | train loss 34453.12643 | val loss 1357.58926 | lr 0.09411920 | r2 sklearn: 0.04280 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   5 | 26.33 s | train loss 34383.32050 | val loss 1337.42840 | lr 0.09223682 | r2 sklearn: 0.05702 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   6 | 26.29 s | train loss 34337.69743 | val loss 1321.75335 | lr 0.09039208 | r2 sklearn: 0.06807 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   7 | 26.42 s | train loss 34267.13371 | val loss 1289.01043 | lr 0.08858424 | r2 sklearn: 0.09115 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   8 | 26.36 s | train loss 34128.53652 | val loss 1212.37592 | lr 0.08681255 | r2 sklearn: 0.14519 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   9 | 26.31 s | train loss 33783.71761 | val loss 1121.34169 | lr 0.08507630 | r2 sklearn: 0.20937 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  10 | 26.22 s | train loss 33484.94001 | val loss 1075.49805 | lr 0.08337478 | r2 sklearn: 0.24170 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  11 | 26.31 s | train loss 33291.88851 | val loss 1050.54726 | lr 0.08170728 | r2 sklearn: 0.25929 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  12 | 26.26 s | train loss 33224.02925 | val loss 1043.36450 | lr 0.08007314 | r2 sklearn: 0.26435 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  13 | 26.24 s | train loss 33150.64551 | val loss 1043.15259 | lr 0.07847167 | r2 sklearn: 0.26450 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  14 | 26.31 s | train loss 33125.95035 | val loss 1038.07050 | lr 0.07690224 | r2 sklearn: 0.26808 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  15 | 26.23 s | train loss 33033.26571 | val loss 1029.26473 | lr 0.07536419 | r2 sklearn: 0.27429 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  16 | 26.40 s | train loss 32976.41272 | val loss 1023.17595 | lr 0.07385691 | r2 sklearn: 0.27859 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  17 | 26.29 s | train loss 32968.22569 | val loss 1023.32303 | lr 0.07237977 | r2 sklearn: 0.27848 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  18 | 26.34 s | train loss 32924.17194 | val loss 1018.62322 | lr 0.07093218 | r2 sklearn: 0.28180 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  19 | 26.36 s | train loss 32942.00522 | val loss 1017.65449 | lr 0.06951353 | r2 sklearn: 0.28248 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  20 | 26.26 s | train loss 32877.74735 | val loss 1017.56307 | lr 0.06812326 | r2 sklearn: 0.28254 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  21 | 26.35 s | train loss 32884.59555 | val loss 1021.95091 | lr 0.06676080 | r2 sklearn: 0.27945 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  22 | 26.41 s | train loss 32905.33415 | val loss 1008.06798 | lr 0.06542558 | r2 sklearn: 0.28924 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  23 | 26.40 s | train loss 32858.85851 | val loss 1010.67981 | lr 0.06411707 | r2 sklearn: 0.28740 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  24 | 26.27 s | train loss 32775.83085 | val loss 1013.33314 | lr 0.06283473 | r2 sklearn: 0.28553 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  25 | 26.29 s | train loss 32811.16113 | val loss 1008.69856 | lr 0.06157803 | r2 sklearn: 0.28879 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  26 | 26.41 s | train loss 33010.52756 | val loss 1028.59503 | lr 0.06034647 | r2 sklearn: 0.27477 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  27 | 26.28 s | train loss 32800.04712 | val loss 1006.40457 | lr 0.05913954 | r2 sklearn: 0.29041 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  28 | 26.28 s | train loss 32760.11077 | val loss 1009.66292 | lr 0.05795675 | r2 sklearn: 0.28811 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  29 | 26.29 s | train loss 32775.75767 | val loss 1020.20710 | lr 0.05679762 | r2 sklearn: 0.28068 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  30 | 26.30 s | train loss 32827.15883 | val loss 1026.25543 | lr 0.05566167 | r2 sklearn: 0.27642 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  31 | 26.32 s | train loss 32783.11061 | val loss 1014.62781 | lr 0.05454843 | r2 sklearn: 0.28461 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  32 | 26.39 s | train loss 32914.70806 | val loss 1033.93557 | lr 0.05345746 | r2 sklearn: 0.27100 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "Done with prediction len 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaen/miniconda3/envs/lastestorch/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "| epoch   1 | 26.31 s | train loss 36083.66095 | val loss 1967.00389 | lr 0.10000000 | r2 sklearn: 0.01480 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   2 | 26.31 s | train loss 35624.80841 | val loss 1919.30284 | lr 0.09800000 | r2 sklearn: 0.03869 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   3 | 26.37 s | train loss 35265.92599 | val loss 1851.65858 | lr 0.09604000 | r2 sklearn: 0.07257 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   4 | 26.31 s | train loss 35026.55784 | val loss 1817.19211 | lr 0.09411920 | r2 sklearn: 0.08984 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   5 | 26.29 s | train loss 34836.17593 | val loss 1770.51485 | lr 0.09223682 | r2 sklearn: 0.11322 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   6 | 26.26 s | train loss 34497.66331 | val loss 1586.18110 | lr 0.09039208 | r2 sklearn: 0.20554 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   7 | 26.34 s | train loss 33734.85746 | val loss 1464.49042 | lr 0.08858424 | r2 sklearn: 0.26649 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   8 | 26.37 s | train loss 33386.44562 | val loss 1398.97537 | lr 0.08681255 | r2 sklearn: 0.29931 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   9 | 26.29 s | train loss 33075.76850 | val loss 1362.05400 | lr 0.08507630 | r2 sklearn: 0.31780 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  10 | 26.26 s | train loss 32994.25283 | val loss 1360.37130 | lr 0.08337478 | r2 sklearn: 0.31864 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  11 | 26.31 s | train loss 32835.42616 | val loss 1350.33686 | lr 0.08170728 | r2 sklearn: 0.32367 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  12 | 26.26 s | train loss 32935.35601 | val loss 1355.91223 | lr 0.08007314 | r2 sklearn: 0.32087 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  13 | 26.36 s | train loss 32675.51710 | val loss 1323.39687 | lr 0.07847167 | r2 sklearn: 0.33716 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  14 | 26.36 s | train loss 32578.79820 | val loss 1328.47585 | lr 0.07690224 | r2 sklearn: 0.33462 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  15 | 26.35 s | train loss 32715.73486 | val loss 1335.57258 | lr 0.07536419 | r2 sklearn: 0.33106 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  16 | 26.34 s | train loss 32511.44723 | val loss 1318.78110 | lr 0.07385691 | r2 sklearn: 0.33947 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  17 | 26.34 s | train loss 32563.73429 | val loss 1320.64353 | lr 0.07237977 | r2 sklearn: 0.33854 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  18 | 26.38 s | train loss 32860.53021 | val loss 1367.97820 | lr 0.07093218 | r2 sklearn: 0.31483 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  19 | 26.29 s | train loss 32586.22039 | val loss 1322.24282 | lr 0.06951353 | r2 sklearn: 0.33774 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  20 | 26.33 s | train loss 32606.56954 | val loss 1371.31826 | lr 0.06812326 | r2 sklearn: 0.31316 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  21 | 26.31 s | train loss 32532.28281 | val loss 1314.77613 | lr 0.06676080 | r2 sklearn: 0.34148 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  22 | 26.41 s | train loss 32395.89057 | val loss 1311.14352 | lr 0.06542558 | r2 sklearn: 0.34330 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  23 | 26.37 s | train loss 32309.87672 | val loss 1311.87736 | lr 0.06411707 | r2 sklearn: 0.34293 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  24 | 26.37 s | train loss 32296.90686 | val loss 1315.87516 | lr 0.06283473 | r2 sklearn: 0.34093 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  25 | 26.39 s | train loss 32329.10869 | val loss 1318.51050 | lr 0.06157803 | r2 sklearn: 0.33961 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  26 | 26.23 s | train loss 32345.99401 | val loss 1301.05905 | lr 0.06034647 | r2 sklearn: 0.34835 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  27 | 26.37 s | train loss 32250.43056 | val loss 1298.92939 | lr 0.05913954 | r2 sklearn: 0.34942 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  28 | 26.33 s | train loss 32235.53890 | val loss 1317.81045 | lr 0.05795675 | r2 sklearn: 0.33996 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  29 | 26.32 s | train loss 32260.38791 | val loss 1289.24944 | lr 0.05679762 | r2 sklearn: 0.35426 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  30 | 26.25 s | train loss 32202.06371 | val loss 1312.91266 | lr 0.05566167 | r2 sklearn: 0.34241 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  31 | 26.41 s | train loss 32843.89505 | val loss 1290.89857 | lr 0.05454843 | r2 sklearn: 0.35344 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch  32 | 26.34 s | train loss 32223.05300 | val loss 1331.52873 | lr 0.05345746 | r2 sklearn: 0.33309 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "Done with prediction len 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaen/miniconda3/envs/lastestorch/lib/python3.11/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "| epoch   1 | 26.35 s | train loss 36449.98303 | val loss 2467.25752 | lr 0.10000000 | r2 sklearn: -0.00748 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   2 | 26.47 s | train loss 35969.97216 | val loss 2412.31534 | lr 0.09800000 | r2 sklearn: 0.01496 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   3 | 26.22 s | train loss 35683.09008 | val loss 2307.22411 | lr 0.09604000 | r2 sklearn: 0.05787 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   4 | 26.13 s | train loss 35374.37548 | val loss 2147.92080 | lr 0.09411920 | r2 sklearn: 0.12292 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   5 | 26.16 s | train loss 34497.14689 | val loss 1772.19057 | lr 0.09223682 | r2 sklearn: 0.27634 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   6 | 26.10 s | train loss 33644.23362 | val loss 1690.45612 | lr 0.09039208 | r2 sklearn: 0.30972 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------\n",
      "| epoch   7 | 26.28 s | train loss 33154.21659 | val loss 1656.12757 | lr 0.08858424 | r2 sklearn: 0.32374 | b, s, h: 0|\n",
      "----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_name = 'Transwiths4_tmp_1'\n",
    "\n",
    "save_path = os.path.join(f'/home/gaen/Documents/codespace-gaen/Ts-master/playround_models/{model_name}/training_details/HFTransformer/results_HFformer',\n",
    "                            str(int(time.time()))+'_results.csv')\n",
    "\n",
    "# save_path = f'/mnt/workspace/shell/tmp/{model_name}/training_details/HFTransformer/results_HFformer'\n",
    "# # save_path=None\n",
    "# save_path = f'/home/gaen/Documents/codespace-gaen/Simons/{model_name}/training_details/HFTransformer/results_HFformer'\n",
    "filepath = f'/home/gaen/Documents/codespace-gaen/Ts-master{model_name}/training_details/HFTransformer/results_HFformer/HFformer'\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "os.makedirs(filepath, exist_ok=True)\n",
    "\n",
    "forecast_history = 100\n",
    "epochs = 32\n",
    "batch_size = 256 #64 for linear decoder\n",
    "\n",
    "forecast_windows = [i for i in range(1,32)]\n",
    "\n",
    "for forecast_window in forecast_windows:\n",
    "    \n",
    "    # orderbook = augment_trade_data(agg_trade, lag=0, forecast_window=forecast_window)\n",
    "    orderbook = augment_trade_data(agg_trade, lag=0, forecast_window=forecast_window)\n",
    "    # total_orw = orderbook.shape[0]\n",
    "    # print(total_orw)\n",
    "    features = ['price', 'lag_return',\n",
    "                'bid1', 'bidqty1', 'bid2', 'bidqty2', 'bid3', 'bidqty3', 'bid4', 'bidqty4', 'bid5', 'bidqty5',\n",
    "                'bid6', 'bidqty6', 'bid7', 'bidqty7', 'bid8', 'bidqty8', 'bid9', 'bidqty9',\n",
    "                'ask1', 'askqty1', 'ask2', 'askqty2', 'ask3', 'askqty3', 'ask4', 'askqty4', 'ask5', 'askqty5',\n",
    "                'ask6', 'askqty6', 'ask7', 'askqty7', 'ask8', 'askqty8', 'ask9', 'askqty9']\n",
    "\n",
    "\n",
    "    split_index, data_x_train, data_y_train, data_x_val, data_y_val = prepare_data(np.array(orderbook[features][1_000_000:1_350_000]),\n",
    "                                                                                                                            np.array(agg_trade.datetime[899_999:1_000_000]),\n",
    "                                                                                                                            np.array(orderbook[features][60_000:60_600]),\n",
    "                                                                                                                            np.array(agg_trade.datetime[60_000:60_600]),\n",
    "                                                                                                                            config, lag=forecast_window, plot=False)\n",
    "\n",
    "    # data_x_train_shape = data_x_train.shape[1]\n",
    "    # data_y_train_shape = data_y_train.shape[1]\n",
    "    # data_x_val_shape = data_x_val.shape[1]\n",
    "    # data_y_val_shape = data_y_val.shape[1]\n",
    "    # print( data_x_train_shape , data_y_train_shape ,data_x_val_shape, data_y_val_shape)\n",
    "    train_loader = TimeSeriesDataset(data_x_train, data_y_train)\n",
    "    val_loader = TimeSeriesDataset(data_x_val, data_y_val)\n",
    "    test_loader = None\n",
    "\n",
    "    model_custom = S4Transformer(input_size=len(features), d_model=36,\n",
    "                 num_layers=3, output_size=1).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "    optimizer = optim.AdamW(model_custom.parameters(), lr=0.1, amsgrad=True)\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
    "    warmup_scheduler = warmup.LinearWarmup(optimizer, warmup_period=1000)\n",
    "\n",
    "    trainer(model_custom, train_loader, val_loader, test_loader, [criterion], optimizer, scheduler, warmup_scheduler, epochs, batch_size=batch_size,\n",
    "        forecast_horizon=forecast_window, takes_target=False, plot_prediction=False, save_path=save_path, LAG=forecast_window)\n",
    "    \n",
    "    del data_x_train \n",
    "    del data_y_train\n",
    "    del data_x_val\n",
    "    del data_y_val\n",
    "\n",
    "    torch.save(model_custom, f'/home/gaen/Documents/codespace-gaen/Ts-master/playround_models/{model_name}/transformer_enclinear_forecasting_FINAL_horizon_{forecast_window}.pt')\n",
    "    print(f'Done with prediction len {forecast_window}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRVCqRHngARx"
   },
   "source": [
    "## Forecast Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QwYi2KUoQ61u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djC9UZA6gARy"
   },
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UeQr6KHN1HDZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-vAZLp5gARy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "7_HFformerv2.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "lastestorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
